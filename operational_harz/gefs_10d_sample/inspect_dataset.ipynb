{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a6e6ca",
   "metadata": {},
   "source": [
    "# Harz Basin Dataset Inspection\n",
    "\n",
    "This notebook provides a comprehensive analysis of the Harz basin datasets, combining basin centroid extraction from shapefiles with NOAA GEFS forecast data and historical weather data analysis.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The analysis consists of the following components:\n",
    "\n",
    "### 1. Basin Centroid Extraction\n",
    "- Load basin shapefiles from `data/harz/basin_shapefiles/`\n",
    "- Extract centroid coordinates for each basin\n",
    "- Visualize basin locations and centroids\n",
    "\n",
    "### 2. NOAA GEFS Forecast Analysis\n",
    "- Connect to NOAA GEFS 35-day forecast dataset\n",
    "- Extract forecast data for basin centroids using all initialization times\n",
    "- Compute ensemble quartiles (25th, 50th, 75th percentiles)\n",
    "- Interpolate forecast data to hourly resolution\n",
    "\n",
    "### 3. Historical Weather Data\n",
    "- Fetch historical weather data for basin locations\n",
    "- Align temporal coverage with forecast initialization times\n",
    "- Merge historical and forecast data\n",
    "\n",
    "### 4. Data Visualization and Analysis\n",
    "- Compare historical observations with forecast quartiles\n",
    "- Generate publication-quality plots following repository standards\n",
    "- Export processed datasets for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea25bd23",
   "metadata": {},
   "source": [
    "## Import Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea0199ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not import custom modules: No module named 'fetch_basin_forecasts'\n",
      "Some functionality may be limited.\n",
      "Libraries imported successfully.\n",
      "Working directory: /home/sngrj0hn/GitHub/neuralhydrology/operational_harz/gefs_10d_sample\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# Geospatial libraries\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# Set matplotlib style to match repository standards\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman'],\n",
    "    'font.size': 11,\n",
    "    'axes.linewidth': 1.0,\n",
    "    'grid.linewidth': 0.5,\n",
    "    'lines.linewidth': 1.5\n",
    "})\n",
    "\n",
    "# Add src directory to path for custom modules\n",
    "src_path = Path('../src')\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "\n",
    "# Import custom modules\n",
    "try:\n",
    "    from fetch_basin_forecasts import (\n",
    "        load_basin_centroids,\n",
    "        fetch_forecasts_for_basins,\n",
    "        interpolate_to_hourly,\n",
    "    )\n",
    "    from fetch_basin_historical import fetch_historical_for_basins\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: Could not import custom modules: {e}\")\n",
    "    print(\"Some functionality may be limited.\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f7411f",
   "metadata": {},
   "source": [
    "## Define Data Paths and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b51d0bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapefile directory: ../../data/harz/basin_shapefiles\n",
      "Output directory: ../../data/harz/basin_centroids\n",
      "Basin centroids file: ../../data/harz/basin_centroids/basin_centroids.csv\n",
      "Shapefile directory exists: True\n"
     ]
    }
   ],
   "source": [
    "# Data paths\n",
    "shapefile_dir = Path(\"../../data/harz/basin_shapefiles\")\n",
    "output_dir = Path(\"../../data/harz/basin_centroids\")\n",
    "basin_centroids_file = output_dir / \"basin_centroids.csv\"\n",
    "\n",
    "# Create output directory\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Visualization parameters\n",
    "FIG_WIDTH = 6.4\n",
    "FIG_HEIGHT = 5.0\n",
    "DPI = 300\n",
    "\n",
    "# Analysis parameters\n",
    "FORECAST_HOURS = 240  # 10 days for hourly interpolation\n",
    "HISTORICAL_DAYS = 7   # Days of historical data for visualization\n",
    "QUARTILES = [0.25, 0.5, 0.75]  # Ensemble quartiles to compute\n",
    "\n",
    "print(f\"Shapefile directory: {shapefile_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Basin centroids file: {basin_centroids_file}\")\n",
    "print(f\"Shapefile directory exists: {shapefile_dir.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea3cd3",
   "metadata": {},
   "source": [
    "## 1. Basin Centroid Extraction\n",
    "\n",
    "Extract centroid coordinates from basin shapefiles in the Harz dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be68e09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 shapefile(s):\n",
      "  - innerste_reservoir_catchment.shp\n",
      "  - oker_reservoir_catchment.shp\n",
      "  - ecker_reservoir_catchment.shp\n",
      "  - soese_reservoir_catchment.shp\n",
      "  - grane_reservoir_catchment.shp\n",
      "\n",
      "Processing innerste_reservoir_catchment.shp:\n",
      "  - CRS: EPSG:4326\n",
      "  - Number of features: 1\n",
      "  - innerste_reservoir_catchment_Basin_0: (51.8345, 10.3078)\n",
      "\n",
      "Processing oker_reservoir_catchment.shp:\n",
      "  - CRS: EPSG:4326\n",
      "  - Number of features: 1\n",
      "  - oker_reservoir_catchment_Basin_0: (51.8165, 10.4473)\n",
      "\n",
      "Processing ecker_reservoir_catchment.shp:\n",
      "  - CRS: EPSG:4326\n",
      "  - Number of features: 1\n",
      "  - ecker_reservoir_catchment_Basin_0: (51.8101, 10.5841)\n",
      "\n",
      "Processing soese_reservoir_catchment.shp:\n",
      "  - CRS: EPSG:4326\n",
      "  - Number of features: 1\n",
      "  - soese_reservoir_catchment_Basin_0: (51.7523, 10.3832)\n",
      "\n",
      "Processing grane_reservoir_catchment.shp:\n",
      "  - CRS: EPSG:4326\n",
      "  - Number of features: 1\n",
      "  - grane_reservoir_catchment_Basin_0: (51.8844, 10.3572)\n",
      "\n",
      "Centroids saved to: ../../data/harz/basin_centroids/basin_centroids.csv\n",
      "\n",
      "Successfully extracted 5 basin centroids\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "basin",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "longitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "latitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "shapefile",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "5fb08fb2-d6da-4fd7-bc42-be7be8306f59",
       "rows": [
        [
         "0",
         "innerste_reservoir_catchment_Basin_0",
         "10.30782783675973",
         "51.834451203470145",
         "innerste_reservoir_catchment.shp"
        ],
        [
         "1",
         "oker_reservoir_catchment_Basin_0",
         "10.44727599122242",
         "51.81646796763249",
         "oker_reservoir_catchment.shp"
        ],
        [
         "2",
         "ecker_reservoir_catchment_Basin_0",
         "10.584057984299879",
         "51.81005645076132",
         "ecker_reservoir_catchment.shp"
        ],
        [
         "3",
         "soese_reservoir_catchment_Basin_0",
         "10.383224738051503",
         "51.75231022422967",
         "soese_reservoir_catchment.shp"
        ],
        [
         "4",
         "grane_reservoir_catchment_Basin_0",
         "10.357202899653627",
         "51.88439077116868",
         "grane_reservoir_catchment.shp"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basin</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>shapefile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>innerste_reservoir_catchment_Basin_0</td>\n",
       "      <td>10.307828</td>\n",
       "      <td>51.834451</td>\n",
       "      <td>innerste_reservoir_catchment.shp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oker_reservoir_catchment_Basin_0</td>\n",
       "      <td>10.447276</td>\n",
       "      <td>51.816468</td>\n",
       "      <td>oker_reservoir_catchment.shp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ecker_reservoir_catchment_Basin_0</td>\n",
       "      <td>10.584058</td>\n",
       "      <td>51.810056</td>\n",
       "      <td>ecker_reservoir_catchment.shp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>soese_reservoir_catchment_Basin_0</td>\n",
       "      <td>10.383225</td>\n",
       "      <td>51.752310</td>\n",
       "      <td>soese_reservoir_catchment.shp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>grane_reservoir_catchment_Basin_0</td>\n",
       "      <td>10.357203</td>\n",
       "      <td>51.884391</td>\n",
       "      <td>grane_reservoir_catchment.shp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  basin  longitude   latitude  \\\n",
       "0  innerste_reservoir_catchment_Basin_0  10.307828  51.834451   \n",
       "1      oker_reservoir_catchment_Basin_0  10.447276  51.816468   \n",
       "2     ecker_reservoir_catchment_Basin_0  10.584058  51.810056   \n",
       "3     soese_reservoir_catchment_Basin_0  10.383225  51.752310   \n",
       "4     grane_reservoir_catchment_Basin_0  10.357203  51.884391   \n",
       "\n",
       "                          shapefile  \n",
       "0  innerste_reservoir_catchment.shp  \n",
       "1      oker_reservoir_catchment.shp  \n",
       "2     ecker_reservoir_catchment.shp  \n",
       "3     soese_reservoir_catchment.shp  \n",
       "4     grane_reservoir_catchment.shp  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_basin_centroids(shapefile_dir, output_file=None):\n",
    "    \"\"\"\n",
    "    Extract centroids from all shapefiles in the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    shapefile_dir : Path or str\n",
    "        Directory containing basin shapefiles\n",
    "    output_file : Path or str, optional\n",
    "        Output CSV file path for centroids\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    geopandas.GeoDataFrame\n",
    "        Combined data with basin names and centroid coordinates\n",
    "    \"\"\"\n",
    "    shapefile_dir = Path(shapefile_dir)\n",
    "    \n",
    "    if not shapefile_dir.exists():\n",
    "        raise FileNotFoundError(f\"Shapefile directory not found: {shapefile_dir}\")\n",
    "    \n",
    "    # Find all shapefiles\n",
    "    shapefiles = list(shapefile_dir.glob(\"*.shp\"))\n",
    "    \n",
    "    if not shapefiles:\n",
    "        raise FileNotFoundError(f\"No shapefiles found in {shapefile_dir}\")\n",
    "    \n",
    "    print(f\"Found {len(shapefiles)} shapefile(s):\")\n",
    "    for shp in shapefiles:\n",
    "        print(f\"  - {shp.name}\")\n",
    "    \n",
    "    all_centroids = []\n",
    "    \n",
    "    for shapefile_path in shapefiles:\n",
    "        try:\n",
    "            # Read shapefile\n",
    "            gdf = gpd.read_file(shapefile_path)\n",
    "            print(f\"\\nProcessing {shapefile_path.name}:\")\n",
    "            print(f\"  - CRS: {gdf.crs}\")\n",
    "            print(f\"  - Number of features: {len(gdf)}\")\n",
    "            \n",
    "            # Ensure CRS is geographic (WGS84) for lat/lon coordinates\n",
    "            if gdf.crs != 'EPSG:4326':\n",
    "                print(f\"  - Converting from {gdf.crs} to EPSG:4326\")\n",
    "                gdf = gdf.to_crs('EPSG:4326')\n",
    "            \n",
    "            # Calculate centroids\n",
    "            centroids = gdf.geometry.centroid\n",
    "            \n",
    "            # Extract coordinates\n",
    "            lons = centroids.x.values\n",
    "            lats = centroids.y.values\n",
    "            \n",
    "            # Create basin names\n",
    "            basin_base_name = shapefile_path.stem\n",
    "            \n",
    "            for i, (lon, lat) in enumerate(zip(lons, lats)):\n",
    "                basin_name = f\"{basin_base_name}_Basin_{i}\"\n",
    "                all_centroids.append({\n",
    "                    'basin': basin_name,\n",
    "                    'longitude': lon,\n",
    "                    'latitude': lat,\n",
    "                    'shapefile': shapefile_path.name\n",
    "                })\n",
    "                print(f\"  - {basin_name}: ({lat:.4f}, {lon:.4f})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {shapefile_path.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_centroids:\n",
    "        raise ValueError(\"No valid centroids extracted from shapefiles\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    centroids_df = pd.DataFrame(all_centroids)\n",
    "    \n",
    "    # Save to CSV if output file specified\n",
    "    if output_file:\n",
    "        output_file = Path(output_file)\n",
    "        output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        centroids_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nCentroids saved to: {output_file}\")\n",
    "    \n",
    "    return centroids_df\n",
    "\n",
    "# Extract centroids from shapefiles\n",
    "try:\n",
    "    centroids_df = extract_basin_centroids(shapefile_dir, basin_centroids_file)\n",
    "    print(f\"\\nSuccessfully extracted {len(centroids_df)} basin centroids\")\n",
    "    display(centroids_df)\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting centroids: {e}\")\n",
    "    # Create dummy data for demonstration if shapefiles not available\n",
    "    centroids_df = pd.DataFrame({\n",
    "        'basin': ['oker_reservoir_catchment_Basin_0', 'sample_basin_Basin_1'],\n",
    "        'longitude': [10.5, 10.6],\n",
    "        'latitude': [51.8, 51.9],\n",
    "        'shapefile': ['oker_reservoir_catchment.shp', 'sample_basin.shp']\n",
    "    })\n",
    "    print(\"Using dummy centroid data for demonstration\")\n",
    "    display(centroids_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6cb57f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# show random plot to verify matplotlib setup\n",
    "def show_random_plot():\n",
    "    \"\"\"\n",
    "    Show a random plot to verify matplotlib setup.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "    sns.scatterplot(data=centroids_df, x='longitude', y='latitude', hue='basin', palette='viridis')\n",
    "    plt.title('Basin Centroids')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.legend(title='Basin', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_random_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ddb618",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.plot(centroids_df['longitude'], centroids_df['latitude'], 'ro', markersize=5)\n",
    "plt.title('Basin Centroids')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid()\n",
    "plt.savefig(output_dir / 'basin_centroids.png', dpi=DPI, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138a900",
   "metadata": {},
   "source": [
    "## 2. Visualize Basin Locations\n",
    "\n",
    "Create a map showing the basin locations and their centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_basin_overview(centroids_df, shapefile_dir=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Create an overview plot of basin locations and centroids.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    centroids_df : pandas.DataFrame\n",
    "        DataFrame with basin centroids\n",
    "    shapefile_dir : Path or str, optional\n",
    "        Directory with shapefiles for basin boundaries\n",
    "    save_path : Path or str, optional\n",
    "        Path to save the plot\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "    \n",
    "    # Plot basin boundaries if shapefiles available\n",
    "    if shapefile_dir and Path(shapefile_dir).exists():\n",
    "        shapefiles = list(Path(shapefile_dir).glob(\"*.shp\"))\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(shapefiles)))\n",
    "        \n",
    "        for i, shapefile_path in enumerate(shapefiles):\n",
    "            try:\n",
    "                gdf = gpd.read_file(shapefile_path)\n",
    "                if gdf.crs != 'EPSG:4326':\n",
    "                    gdf = gdf.to_crs('EPSG:4326')\n",
    "                \n",
    "                gdf.plot(ax=ax, color=colors[i], alpha=0.6, \n",
    "                         edgecolor='black', linewidth=0.5,\n",
    "                         label=shapefile_path.stem)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not plot {shapefile_path.name}: {e}\")\n",
    "    \n",
    "    # Plot centroids\n",
    "    ax.scatter(centroids_df['longitude'], centroids_df['latitude'], \n",
    "               c='red', s=50, marker='x', linewidth=2, \n",
    "               label='Basin Centroids', zorder=5)\n",
    "    \n",
    "    # Add basin labels\n",
    "    for idx, row in centroids_df.iterrows():\n",
    "        ax.annotate(row['basin'], \n",
    "                   (row['longitude'], row['latitude']),\n",
    "                   xytext=(5, 5), textcoords='offset points',\n",
    "                   fontsize=8, ha='left')\n",
    "    \n",
    "    ax.set_xlabel('Longitude (°E)')\n",
    "    ax.set_ylabel('Latitude (°N)')\n",
    "    ax.set_title('Harz Basin Locations and Centroids')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=DPI, bbox_inches='tight')\n",
    "        print(f\"Basin overview plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create basin overview plot\n",
    "overview_plot_path = output_dir / \"basin_overview.pdf\"\n",
    "# plot_basin_overview(centroids_df, shapefile_dir, overview_plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf734db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_basin_overview(centroids_df, shapefile_dir, overview_plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59cb521",
   "metadata": {},
   "source": [
    "## 3. Load NOAA GEFS Forecast Dataset\n",
    "\n",
    "Connect to the NOAA GEFS 35-day forecast dataset and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NOAA GEFS forecast dataset\n",
    "print(\"Connecting to NOAA GEFS dataset...\")\n",
    "try:\n",
    "    ds = xr.open_zarr(\n",
    "        \"https://data.dynamical.org/noaa/gefs/forecast-35-day/latest.zarr?email=optional@email.com\", \n",
    "        decode_timedelta=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Dataset Summary ===\")\n",
    "    print(f\"Time domain: {ds.attrs.get('time_domain', 'N/A')}\")\n",
    "    print(f\"Forecast domain: {ds.attrs.get('forecast_domain', 'N/A')}\")\n",
    "    print(f\"Spatial resolution: {ds.attrs.get('spatial_resolution', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\n=== Dataset Dimensions ===\")\n",
    "    for dim, size in ds.dims.items():\n",
    "        print(f\"  {dim}: {size}\")\n",
    "    \n",
    "    print(\"\\n=== Variables ===\")\n",
    "    for var in list(ds.data_vars)[:5]:  # Show first 5 variables\n",
    "        print(f\"  {var}: {ds[var].dims}\")\n",
    "    if len(ds.data_vars) > 5:\n",
    "        print(f\"  ... and {len(ds.data_vars) - 5} more variables\")\n",
    "    \n",
    "    # Show initialization times\n",
    "    init_times = pd.to_datetime(ds.init_time.values)\n",
    "    print(f\"\\n=== Initialization Times ===\")\n",
    "    print(f\"Total number: {len(init_times)}\")\n",
    "    print(f\"First: {init_times[0].strftime('%Y-%m-%d %H:%M')}\")\n",
    "    print(f\"Last: {init_times[-1].strftime('%Y-%m-%d %H:%M')}\")\n",
    "    \n",
    "    print(\"\\nDataset loaded successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading NOAA GEFS dataset: {e}\")\n",
    "    print(\"Proceeding with limited analysis.\")\n",
    "    ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd383b10",
   "metadata": {},
   "source": [
    "## 4. Extract Forecasts for Basin Centroids\n",
    "\n",
    "Extract forecast data for each basin centroid location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7edaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract forecasts for basin centroids\n",
    "if ds is not None:\n",
    "    try:\n",
    "        print(\"Extracting forecasts for basin centroids...\")\n",
    "        \n",
    "        # Prepare centroids data structure expected by fetch_forecasts_for_basins\n",
    "        if 'load_basin_centroids' in globals():\n",
    "            # Use existing function if available\n",
    "            centroids = centroids_df\n",
    "        else:\n",
    "            # Prepare data structure manually\n",
    "            centroids = centroids_df\n",
    "        \n",
    "        # Extract forecasts (use all initialization times)\n",
    "        if 'fetch_forecasts_for_basins' in globals():\n",
    "            basin_forecasts = fetch_forecasts_for_basins(ds, centroids, init_time=None)\n",
    "            print(f\"Successfully extracted forecasts for {len(basin_forecasts.basin)} basins\")\n",
    "            print(f\"Forecast dimensions: {dict(basin_forecasts.dims)}\")\n",
    "        else:\n",
    "            print(\"fetch_forecasts_for_basins function not available\")\n",
    "            basin_forecasts = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting forecasts: {e}\")\n",
    "        basin_forecasts = None\n",
    "else:\n",
    "    print(\"Skipping forecast extraction - dataset not available\")\n",
    "    basin_forecasts = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2710b1",
   "metadata": {},
   "source": [
    "## 5. Compute Forecast Quartiles\n",
    "\n",
    "Convert ensemble forecasts to quartiles to reduce data size while preserving statistical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced9965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forecast_quartiles_as_variables(forecast_ds, quartiles=None):\n",
    "    \"\"\"\n",
    "    Compute quartiles from ensemble forecast data as separate variables.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    forecast_ds : xarray.Dataset\n",
    "        Dataset with ensemble_member dimension\n",
    "    quartiles : list, optional\n",
    "        List of quantiles to compute (default: [0.25, 0.5, 0.75])\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Dataset with separate variables for each quartile\n",
    "    \"\"\"\n",
    "    if quartiles is None:\n",
    "        quartiles = QUARTILES\n",
    "    \n",
    "    print(f\"Computing quartiles {quartiles} from ensemble forecasts...\")\n",
    "    \n",
    "    # Define quartile suffixes\n",
    "    quartile_suffixes = {\n",
    "        0.25: '_q25',\n",
    "        0.5: '_q50', \n",
    "        0.75: '_q75'\n",
    "    }\n",
    "    \n",
    "    new_data_vars = {}\n",
    "    \n",
    "    # Process each data variable\n",
    "    for var_name in forecast_ds.data_vars:\n",
    "        print(f\"  Processing variable: {var_name}\")\n",
    "        var_data = forecast_ds[var_name]\n",
    "        \n",
    "        # Compute quartiles\n",
    "        var_quartiles = var_data.quantile(quartiles, dim='ensemble_member')\n",
    "        \n",
    "        # Create separate variables for each quartile\n",
    "        for i, q in enumerate(quartiles):\n",
    "            suffix = quartile_suffixes.get(q, f'_q{int(q*100)}')\n",
    "            new_var_name = f\"{var_name}{suffix}\"\n",
    "            \n",
    "            # Extract quartile data (remove quantile dimension)\n",
    "            quartile_data = var_quartiles.isel(quantile=i).drop('quantile')\n",
    "            new_data_vars[new_var_name] = quartile_data\n",
    "    \n",
    "    # Create new dataset with same coordinates (excluding ensemble_member)\n",
    "    coords_to_keep = {k: v for k, v in forecast_ds.coords.items() \n",
    "                      if 'ensemble_member' not in v.dims}\n",
    "    \n",
    "    quartile_ds = xr.Dataset(\n",
    "        data_vars=new_data_vars,\n",
    "        coords=coords_to_keep,\n",
    "        attrs=forecast_ds.attrs.copy()\n",
    "    )\n",
    "    \n",
    "    # Update attributes\n",
    "    quartile_ds.attrs['quartile_processing'] = f'Computed quartiles {quartiles}'\n",
    "    quartile_ds.attrs['original_ensemble_members'] = len(forecast_ds.ensemble_member)\n",
    "    \n",
    "    print(f\"Original dataset dimensions: {dict(forecast_ds.dims)}\")\n",
    "    print(f\"Quartile dataset dimensions: {dict(quartile_ds.dims)}\")\n",
    "    print(f\"Original variables: {len(forecast_ds.data_vars)}\")\n",
    "    print(f\"New quartile variables: {len(quartile_ds.data_vars)}\")\n",
    "    \n",
    "    return quartile_ds\n",
    "\n",
    "# Compute forecast quartiles\n",
    "if basin_forecasts is not None:\n",
    "    try:\n",
    "        basin_forecasts_quartiles = compute_forecast_quartiles_as_variables(basin_forecasts)\n",
    "        print(\"\\nForecast quartiles computed successfully.\")\n",
    "        \n",
    "        # Show sample variables\n",
    "        sample_vars = list(basin_forecasts_quartiles.data_vars)[:6]\n",
    "        print(f\"Sample quartile variables: {sample_vars}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error computing forecast quartiles: {e}\")\n",
    "        basin_forecasts_quartiles = None\n",
    "else:\n",
    "    print(\"Skipping quartile computation - forecast data not available\")\n",
    "    basin_forecasts_quartiles = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b232db4b",
   "metadata": {},
   "source": [
    "## 6. Interpolate to Hourly Resolution\n",
    "\n",
    "Interpolate forecast data to hourly resolution for the first 10 days to enable merging with hourly historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da50789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate forecast quartiles to hourly resolution\n",
    "if basin_forecasts_quartiles is not None:\n",
    "    try:\n",
    "        print(f\"Interpolating forecast data to hourly resolution ({FORECAST_HOURS} hours)...\")\n",
    "        \n",
    "        if 'interpolate_to_hourly' in globals():\n",
    "            basin_forecasts_quartiles_hourly = interpolate_to_hourly(\n",
    "                basin_forecasts_quartiles, max_hours=FORECAST_HOURS\n",
    "            )\n",
    "            \n",
    "            print(f\"Original lead_times: {len(basin_forecasts_quartiles.lead_time)} steps\")\n",
    "            print(f\"Hourly lead_times: {len(basin_forecasts_quartiles_hourly.lead_time)} steps\")\n",
    "            \n",
    "            print(\"\\nInterpolation completed successfully.\")\n",
    "        else:\n",
    "            print(\"interpolate_to_hourly function not available\")\n",
    "            basin_forecasts_quartiles_hourly = basin_forecasts_quartiles\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error interpolating to hourly resolution: {e}\")\n",
    "        basin_forecasts_quartiles_hourly = basin_forecasts_quartiles\n",
    "else:\n",
    "    print(\"Skipping interpolation - quartile data not available\")\n",
    "    basin_forecasts_quartiles_hourly = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f06cfea",
   "metadata": {},
   "source": [
    "## 7. Fetch Historical Weather Data\n",
    "\n",
    "Retrieve historical weather data for the same basin locations to compare with forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11fc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch historical weather data\n",
    "if basin_forecasts_quartiles is not None:\n",
    "    try:\n",
    "        # Determine date range from forecast data\n",
    "        start_date_hist = pd.to_datetime(\n",
    "            basin_forecasts_quartiles.init_time.min().values\n",
    "        ).strftime('%Y-%m-%d')\n",
    "        end_date_hist = pd.to_datetime(\n",
    "            basin_forecasts_quartiles.init_time.max().values\n",
    "        ).strftime('%Y-%m-%d')\n",
    "        \n",
    "        print(f\"Historical data range: {start_date_hist} to {end_date_hist}\")\n",
    "        \n",
    "        # Define historical variables\n",
    "        historical_variables = [\n",
    "            \"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \n",
    "            \"rain\", \"snowfall\", \"soil_moisture_0_to_7cm\", \n",
    "            \"soil_moisture_7_to_28cm\", \"soil_moisture_28_to_100cm\", \n",
    "            \"soil_moisture_100_to_255cm\", \"et0_fao_evapotranspiration\", \n",
    "            \"surface_pressure\", \"snow_depth_water_equivalent\"\n",
    "        ]\n",
    "        \n",
    "        # Fetch historical data\n",
    "        if 'fetch_historical_for_basins' in globals():\n",
    "            basin_historical_data = fetch_historical_for_basins(\n",
    "                centroids_df, start_date_hist, end_date_hist, historical_variables\n",
    "            )\n",
    "            \n",
    "            if basin_historical_data is not None:\n",
    "                print(f\"Historical data dimensions: {dict(basin_historical_data.dims)}\")\n",
    "                print(f\"Historical variables: {list(basin_historical_data.data_vars)[:5]}...\")\n",
    "            else:\n",
    "                print(\"Failed to fetch historical data\")\n",
    "        else:\n",
    "            print(\"fetch_historical_for_basins function not available\")\n",
    "            basin_historical_data = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching historical data: {e}\")\n",
    "        basin_historical_data = None\n",
    "else:\n",
    "    print(\"Skipping historical data fetch - forecast data not available\")\n",
    "    basin_historical_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c44168",
   "metadata": {},
   "source": [
    "## 8. Merge Historical and Forecast Data\n",
    "\n",
    "Combine historical observations with forecast quartiles into a unified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd4922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge historical and forecast data\n",
    "basin_combined_data = None\n",
    "\n",
    "if basin_forecasts_quartiles_hourly is not None:\n",
    "    try:\n",
    "        # Rename 'init_time' to 'time' for consistency\n",
    "        basin_forecasts_renamed = basin_forecasts_quartiles_hourly.rename({'init_time': 'time'})\n",
    "        \n",
    "        if basin_historical_data is not None:\n",
    "            print(\"Merging historical and forecast data...\")\n",
    "            \n",
    "            # Add chunking for historical data\n",
    "            basin_historical_data = basin_historical_data.chunk({\n",
    "                'time': 'auto', 'basin': 'auto'\n",
    "            })\n",
    "            \n",
    "            # Identify conflicting variable names\n",
    "            forecast_base_vars = set()\n",
    "            for var in basin_forecasts_renamed.data_vars:\n",
    "                if var.endswith(('_q25', '_q50', '_q75')):\n",
    "                    base_var = var.rsplit('_q', 1)[0]\n",
    "                    forecast_base_vars.add(base_var)\n",
    "            \n",
    "            conflicting_vars = [\n",
    "                var for var in basin_historical_data.data_vars \n",
    "                if var in forecast_base_vars\n",
    "            ]\n",
    "            \n",
    "            # Rename conflicting historical variables\n",
    "            rename_dict = {var: f\"{var}_hist\" for var in conflicting_vars}\n",
    "            basin_historical_renamed = basin_historical_data.rename(rename_dict)\n",
    "            \n",
    "            if rename_dict:\n",
    "                print(f\"Renamed conflicting variables: {list(rename_dict.values())}\")\n",
    "            \n",
    "            # Merge datasets\n",
    "            basin_combined_data = xr.merge(\n",
    "                [basin_historical_renamed, basin_forecasts_renamed], \n",
    "                compat='no_conflicts'\n",
    "            )\n",
    "            \n",
    "            print(f\"Combined dataset dimensions: {dict(basin_combined_data.dims)}\")\n",
    "            \n",
    "            # Categorize variables by dimensions\n",
    "            historical_vars = []\n",
    "            forecast_vars = []\n",
    "            \n",
    "            for var_name in basin_combined_data.data_vars:\n",
    "                var_dims = basin_combined_data[var_name].dims\n",
    "                if 'lead_time' in var_dims:\n",
    "                    forecast_vars.append(var_name)\n",
    "                else:\n",
    "                    historical_vars.append(var_name)\n",
    "            \n",
    "            print(f\"Historical variables: {len(historical_vars)}\")\n",
    "            print(f\"Forecast variables: {len(forecast_vars)}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Using forecast data only (no historical data available)\")\n",
    "            basin_combined_data = basin_forecasts_renamed\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error merging data: {e}\")\n",
    "        basin_combined_data = None\n",
    "else:\n",
    "    print(\"No data available for merging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d044ca",
   "metadata": {},
   "source": [
    "## 9. Visualize Combined Dataset\n",
    "\n",
    "Create publication-quality plots comparing historical observations with forecast quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bcc9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_forecast_comparison_plot(combined_data, basin_name=None, \n",
    "                                   forecast_init_time=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a comparison plot of historical vs forecast data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    combined_data : xarray.Dataset\n",
    "        Combined historical and forecast dataset\n",
    "    basin_name : str, optional\n",
    "        Basin to plot (if None, uses first basin)\n",
    "    forecast_init_time : str or pd.Timestamp, optional\n",
    "        Forecast initialization time (if None, uses latest)\n",
    "    save_path : Path or str, optional\n",
    "        Path to save the plot\n",
    "    \"\"\"\n",
    "    if combined_data is None:\n",
    "        print(\"No combined data available for plotting\")\n",
    "        return\n",
    "    \n",
    "    # Select basin\n",
    "    if basin_name is None:\n",
    "        basin_name = combined_data.basin.values[0]\n",
    "    \n",
    "    print(f\"Creating plot for basin: {basin_name}\")\n",
    "    \n",
    "    # Select forecast initialization time\n",
    "    if forecast_init_time is None:\n",
    "        # Use latest available time\n",
    "        available_times = pd.to_datetime(combined_data.time.values)\n",
    "        forecast_init_time = available_times[-1]\n",
    "    else:\n",
    "        forecast_init_time = pd.to_datetime(forecast_init_time)\n",
    "    \n",
    "    print(f\"Using forecast initialization time: {forecast_init_time}\")\n",
    "    \n",
    "    try:\n",
    "        # Select data for basin\n",
    "        basin_data = combined_data.sel(basin=basin_name)\n",
    "        \n",
    "        # Define time windows\n",
    "        hist_end_time = forecast_init_time\n",
    "        hist_start_time = hist_end_time - timedelta(days=HISTORICAL_DAYS)\n",
    "        \n",
    "        # Extract data slices\n",
    "        historical_slice = basin_data.sel(time=slice(hist_start_time, hist_end_time))\n",
    "        forecast_slice = basin_data.sel(time=forecast_init_time)\n",
    "        \n",
    "        # Create plot\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(FIG_WIDTH, FIG_HEIGHT), sharex=True)\n",
    "        \n",
    "        # Temperature plot\n",
    "        ax1 = axes[0]\n",
    "        \n",
    "        # Historical temperature\n",
    "        if 'temperature_2m_hist' in historical_slice:\n",
    "            hist_temp = historical_slice['temperature_2m_hist'].dropna(dim='time')\n",
    "            if hist_temp.size > 0:\n",
    "                ax1.plot(hist_temp.time, hist_temp.values, 'k-', \n",
    "                        linewidth=1.5, label='Historical', zorder=5)\n",
    "        \n",
    "        # Forecast temperature quartiles\n",
    "        temp_vars = ['temperature_2m_q25', 'temperature_2m_q50', 'temperature_2m_q75']\n",
    "        if all(var in forecast_slice for var in temp_vars):\n",
    "            # Calculate forecast time axis\n",
    "            lead_hours = forecast_slice.lead_time.values\n",
    "            forecast_times = forecast_init_time + pd.to_timedelta(lead_hours, unit='h')\n",
    "            \n",
    "            # Plot quartile band\n",
    "            ax1.fill_between(forecast_times,\n",
    "                           forecast_slice['temperature_2m_q25'].values,\n",
    "                           forecast_slice['temperature_2m_q75'].values,\n",
    "                           color='#d62728', alpha=0.3, \n",
    "                           label='Forecast IQR (25-75%)', zorder=2)\n",
    "            \n",
    "            # Plot median\n",
    "            ax1.plot(forecast_times, forecast_slice['temperature_2m_q50'].values,\n",
    "                    'r-', linewidth=1.5, label='Forecast Median', zorder=3)\n",
    "        \n",
    "        ax1.set_ylabel('Temperature (°C)')\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.axvline(forecast_init_time, color='blue', linestyle='-', \n",
    "                   linewidth=1, label='Forecast Start', zorder=6)\n",
    "        \n",
    "        # Precipitation plot\n",
    "        ax2 = axes[1]\n",
    "        \n",
    "        # Historical precipitation\n",
    "        if 'precipitation' in historical_slice:\n",
    "            hist_precip = historical_slice['precipitation'].dropna(dim='time')\n",
    "            if hist_precip.size > 0:\n",
    "                # Convert to bars\n",
    "                bar_width = 1/24  # 1 hour in days\n",
    "                ax2.bar(hist_precip.time, hist_precip.values,\n",
    "                       width=bar_width, color='black', alpha=0.7,\n",
    "                       label='Historical', align='edge', zorder=3)\n",
    "        \n",
    "        # Forecast precipitation quartiles\n",
    "        precip_vars = ['precipitation_surface_q25', 'precipitation_surface_q50', \n",
    "                      'precipitation_surface_q75']\n",
    "        if all(var in forecast_slice for var in precip_vars):\n",
    "            # Convert from mm/s to mm/hr\n",
    "            conversion_factor = 3600\n",
    "            \n",
    "            # Plot as bars\n",
    "            bar_width = 1/24  # 1 hour in days\n",
    "            \n",
    "            # Plot median precipitation\n",
    "            precip_median = forecast_slice['precipitation_surface_q50'].values * conversion_factor\n",
    "            ax2.bar(forecast_times, precip_median,\n",
    "                   width=bar_width, color='#1f77b4', alpha=0.8,\n",
    "                   label='Forecast Median', align='edge', zorder=2)\n",
    "        \n",
    "        ax2.set_ylabel('Precipitation (mm)')\n",
    "        ax2.set_xlabel('Time (UTC)')\n",
    "        ax2.legend(loc='upper left')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.axvline(forecast_init_time, color='blue', linestyle='-', \n",
    "                   linewidth=1, zorder=4)\n",
    "        \n",
    "        # Format x-axis\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Add title\n",
    "        fig.suptitle(f'Historical vs Forecast Comparison\\nBasin: {basin_name}', \n",
    "                    y=0.98, fontsize=12)\n",
    "        \n",
    "        # Save plot\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=DPI, bbox_inches='tight')\n",
    "            print(f\"Plot saved to: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating plot: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Create comparison plot\n",
    "if basin_combined_data is not None:\n",
    "    comparison_plot_path = output_dir / \"forecast_comparison.pdf\"\n",
    "    create_forecast_comparison_plot(\n",
    "        basin_combined_data, \n",
    "        save_path=comparison_plot_path\n",
    "    )\n",
    "else:\n",
    "    print(\"No combined data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a0d58",
   "metadata": {},
   "source": [
    "## 10. Data Export and Summary\n",
    "\n",
    "Export processed datasets and provide analysis summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export processed datasets\n",
    "def export_datasets(basin_combined_data, centroids_df, output_dir):\n",
    "    \"\"\"\n",
    "    Export processed datasets to files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    basin_combined_data : xarray.Dataset\n",
    "        Combined historical and forecast data\n",
    "    centroids_df : pandas.DataFrame\n",
    "        Basin centroid coordinates\n",
    "    output_dir : Path\n",
    "        Output directory\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    # Export centroids\n",
    "    centroids_file = output_dir / \"basin_centroids.csv\"\n",
    "    centroids_df.to_csv(centroids_file, index=False)\n",
    "    print(f\"Basin centroids exported to: {centroids_file}\")\n",
    "    \n",
    "    # Export combined dataset if available\n",
    "    if basin_combined_data is not None:\n",
    "        try:\n",
    "            combined_file = output_dir / \"basin_combined_data.nc\"\n",
    "            basin_combined_data.to_netcdf(combined_file)\n",
    "            print(f\"Combined dataset exported to: {combined_file}\")\n",
    "            \n",
    "            # Export dataset summary\n",
    "            summary_file = output_dir / \"dataset_summary.txt\"\n",
    "            with open(summary_file, 'w') as f:\n",
    "                f.write(\"Harz Basin Dataset Summary\\n\")\n",
    "                f.write(\"=\" * 30 + \"\\n\\n\")\n",
    "                f.write(f\"Number of basins: {len(basin_combined_data.basin)}\\n\")\n",
    "                f.write(f\"Dataset dimensions: {dict(basin_combined_data.dims)}\\n\\n\")\n",
    "                \n",
    "                # Categorize variables\n",
    "                historical_vars = []\n",
    "                forecast_vars = []\n",
    "                \n",
    "                for var_name in basin_combined_data.data_vars:\n",
    "                    var_dims = basin_combined_data[var_name].dims\n",
    "                    if 'lead_time' in var_dims:\n",
    "                        forecast_vars.append(var_name)\n",
    "                    else:\n",
    "                        historical_vars.append(var_name)\n",
    "                \n",
    "                f.write(f\"Historical variables ({len(historical_vars)}):\")\n",
    "                for var in historical_vars:\n",
    "                    f.write(f\"\\n  - {var}\")\n",
    "                \n",
    "                f.write(f\"\\n\\nForecast variables ({len(forecast_vars)}):\")\n",
    "                for var in forecast_vars:\n",
    "                    f.write(f\"\\n  - {var}\")\n",
    "                \n",
    "                # Time range\n",
    "                time_range = basin_combined_data.time\n",
    "                f.write(f\"\\n\\nTime range: {time_range.min().values} to {time_range.max().values}\")\n",
    "                \n",
    "                if 'lead_time' in basin_combined_data.dims:\n",
    "                    lead_range = basin_combined_data.lead_time\n",
    "                    f.write(f\"\\nLead time range: {lead_range.min().values} to {lead_range.max().values} hours\")\n",
    "            \n",
    "            print(f\"Dataset summary exported to: {summary_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error exporting combined dataset: {e}\")\n",
    "    else:\n",
    "        print(\"No combined dataset available for export\")\n",
    "\n",
    "# Export datasets\n",
    "print(\"\\n=== Data Export ===\")\n",
    "export_datasets(basin_combined_data, centroids_df, output_dir)\n",
    "\n",
    "# Analysis summary\n",
    "print(\"\\n=== Analysis Summary ===\")\n",
    "print(f\"✓ Basin centroids: {len(centroids_df)} basins extracted\")\n",
    "print(f\"✓ Shapefiles processed: {centroids_df['shapefile'].nunique()} files\")\n",
    "\n",
    "if basin_combined_data is not None:\n",
    "    print(f\"✓ Combined dataset: {dict(basin_combined_data.dims)}\")\n",
    "    print(f\"✓ Variables: {len(basin_combined_data.data_vars)} total\")\n",
    "    print(f\"✓ Time range: {basin_combined_data.time.min().values} to {basin_combined_data.time.max().values}\")\n",
    "else:\n",
    "    print(\"⚠ Combined dataset: Not available (forecast/historical data issues)\")\n",
    "\n",
    "print(f\"\\n📁 Output files saved to: {output_dir}\")\n",
    "print(\"\\n🎯 Analysis complete! The dataset is ready for hydrological modeling.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralhydrology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
