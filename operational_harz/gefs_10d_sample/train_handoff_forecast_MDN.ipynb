{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f71bf584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/sngrj0hn/GitHub/neuralhydrology/operational_harz/gefs_10d_sample\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import glob\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "from neuralhydrology.utils.config import Config\n",
    "from neuralhydrology.datasetzoo.onlineforecastdataset import OnlineForecastDataset\n",
    "from neuralhydrology.nh_run import start_run, eval_run\n",
    "from neuralhydrology.training.train import start_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b3fa65",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc518e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config for experiment: development_run_mdn\n",
      "Run directory: None\n"
     ]
    }
   ],
   "source": [
    "config_path = Path(\"basins_MDN.yml\")\n",
    "cfg = Config(config_path)\n",
    "print(f\"Loaded config for experiment: {cfg.experiment_name}\")\n",
    "print(f\"Run directory: {cfg.run_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32d797",
   "metadata": {},
   "source": [
    "## Inspect Data Loading\n",
    "This step manually instantiates the `OnlineForecastDataset` to verify that:\n",
    "1. The cached `train_data.zarr` can be accessed or rebuilt.\n",
    "2. The scaler is correctly loaded or created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23102fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load OnlineForecastDataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sngrj0hn/anaconda3/envs/neuralhydrology/lib/python3.12/site-packages/zarr/codecs/vlen_utf8.py:44: UserWarning: The codec `vlen-utf8` is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  return cls(**configuration_parsed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  2.65it/s]\n",
      "\n",
      "Successfully loaded OnlineForecastDataset!\n",
      "Number of samples: 5475\n",
      "Scaler keys: ['xarray_feature_scale', 'xarray_feature_center']\n",
      "Zarr cache successfully created at runs/train_data.zarr\n",
      "Successfully loaded OnlineForecastDataset!\n",
      "Number of samples: 5475\n",
      "Scaler keys: ['xarray_feature_scale', 'xarray_feature_center']\n",
      "Zarr cache successfully created at runs/train_data.zarr\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dataset in training mode\n",
    "# This will trigger _load_or_create_xarray_dataset and _ensure_scaler\n",
    "try:\n",
    "    print(\"Attempting to load OnlineForecastDataset...\")\n",
    "    ds = OnlineForecastDataset(cfg=cfg, is_train=True, period=\"train\")\n",
    "    print(\"Successfully loaded OnlineForecastDataset!\")\n",
    "    print(f\"Number of samples: {len(ds)}\")\n",
    "    print(f\"Scaler keys: {list(ds.scaler.keys())}\")\n",
    "\n",
    "    # Verify Zarr cache exists\n",
    "    zarr_path = cfg.train_dir / \"train_data.zarr\"\n",
    "    if zarr_path.exists():\n",
    "        print(f\"Zarr cache successfully created at {zarr_path}\")\n",
    "    else:\n",
    "        print(\"Warning: Zarr cache not found!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253025d4",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "If the data loading above succeeded, we can proceed to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e2c9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing Zarr cache at: runs/train_data.zarr\n",
      "Starting run on GPU: NVIDIA GeForce RTX 4090\n",
      "2025-12-02 12:27:59,256: Logging to /home/sngrj0hn/GitHub/neuralhydrology/operational_harz/gefs_10d_sample/runs/development_run_mdn_0212_122759/output.log initialized.\n",
      "2025-12-02 12:27:59,256: ### Folder structure created at /home/sngrj0hn/GitHub/neuralhydrology/operational_harz/gefs_10d_sample/runs/development_run_mdn_0212_122759\n",
      "2025-12-02 12:27:59,256: ### Run configurations for development_run_mdn\n",
      "2025-12-02 12:27:59,256: experiment_name: development_run_mdn\n",
      "2025-12-02 12:27:59,256: run_dir: /home/sngrj0hn/GitHub/neuralhydrology/operational_harz/gefs_10d_sample/runs/development_run_mdn_0212_122759\n",
      "2025-12-02 12:27:59,256: train_basin_file: basins.txt\n",
      "2025-12-02 12:27:59,257: validation_basin_file: basins.txt\n",
      "2025-12-02 12:27:59,257: test_basin_file: basins.txt\n",
      "2025-12-02 12:27:59,257: train_start_date: 2020-10-01 00:00:00\n",
      "2025-12-02 12:27:59,257: train_end_date: 2023-09-30 00:00:00\n",
      "2025-12-02 12:27:59,257: validation_start_date: 2023-10-01 00:00:00\n",
      "2025-12-02 12:27:59,257: validation_end_date: 2024-01-31 00:00:00\n",
      "2025-12-02 12:27:59,258: test_start_date: 2024-02-01 00:00:00\n",
      "2025-12-02 12:27:59,258: test_end_date: 2024-04-30 00:00:00\n",
      "2025-12-02 12:27:59,258: per_basin_train_periods_file: None\n",
      "2025-12-02 12:27:59,258: per_basin_validation_periods_file: None\n",
      "2025-12-02 12:27:59,258: per_basin_test_periods_file: None\n",
      "2025-12-02 12:27:59,259: seed: 32\n",
      "2025-12-02 12:27:59,259: device: cuda:0\n",
      "2025-12-02 12:27:59,259: validate_every: 4\n",
      "2025-12-02 12:27:59,259: validate_n_random_basins: 4\n",
      "2025-12-02 12:27:59,259: cache_validation_data: True\n",
      "2025-12-02 12:27:59,259: metrics: ['NSE', 'KGE', 'Alpha-NSE', 'Beta-NSE']\n",
      "2025-12-02 12:27:59,260: model: handoff_forecast_lstm\n",
      "2025-12-02 12:27:59,260: checkpoint_path: None\n",
      "2025-12-02 12:27:59,260: head: cmal\n",
      "2025-12-02 12:27:59,260: n_distributions: 3\n",
      "2025-12-02 12:27:59,260: n_samples: 100\n",
      "2025-12-02 12:27:59,260: negative_sample_handling: clip\n",
      "2025-12-02 12:27:59,261: dynamics_embedding: {'type': 'fc', 'hiddens': 64, 'activation': 'tanh', 'dropout': 0.0}\n",
      "2025-12-02 12:27:59,261: shared_mtslstm: False\n",
      "2025-12-02 12:27:59,261: transfer_mtslstm_states: {'h': 'identity', 'c': 'identity'}\n",
      "2025-12-02 12:27:59,261: ode_method: euler\n",
      "2025-12-02 12:27:59,261: ode_num_unfolds: 4\n",
      "2025-12-02 12:27:59,261: ode_random_freq_lower_bound: 6D\n",
      "2025-12-02 12:27:59,262: transformer_nlayers: 4\n",
      "2025-12-02 12:27:59,262: transformer_positional_encoding_type: sum\n",
      "2025-12-02 12:27:59,262: transformer_dim_feedforward: 32\n",
      "2025-12-02 12:27:59,262: transformer_positional_dropout: 0.0\n",
      "2025-12-02 12:27:59,262: transformer_dropout: 0\n",
      "2025-12-02 12:27:59,262: transformer_nheads: 4\n",
      "2025-12-02 12:27:59,263: hidden_size: 128\n",
      "2025-12-02 12:27:59,263: initial_forget_bias: 3\n",
      "2025-12-02 12:27:59,263: output_dropout: 0.4\n",
      "2025-12-02 12:27:59,263: optimizer: Adam\n",
      "2025-12-02 12:27:59,263: loss: CMALLoss\n",
      "2025-12-02 12:27:59,263: regularization: None\n",
      "2025-12-02 12:27:59,263: learning_rate: {0: 0.001, 40: 0.0005, 60: 0.0001}\n",
      "2025-12-02 12:27:59,264: batch_size: 256\n",
      "2025-12-02 12:27:59,264: epochs: 80\n",
      "2025-12-02 12:27:59,264: target_noise_std: 0.005\n",
      "2025-12-02 12:27:59,264: clip_gradient_norm: 1\n",
      "2025-12-02 12:27:59,264: predict_last_n: 240\n",
      "2025-12-02 12:27:59,264: seq_length: 960\n",
      "2025-12-02 12:27:59,264: forecast_seq_length: 240\n",
      "2025-12-02 12:27:59,264: forecast_offset: 0\n",
      "2025-12-02 12:27:59,265: num_workers: 0\n",
      "2025-12-02 12:27:59,265: log_interval: 5\n",
      "2025-12-02 12:27:59,265: log_tensorboard: True\n",
      "2025-12-02 12:27:59,265: log_n_figures: 2\n",
      "2025-12-02 12:27:59,265: save_weights_every: 1\n",
      "2025-12-02 12:27:59,265: save_validation_results: False\n",
      "2025-12-02 12:27:59,265: dataset: online_forecast\n",
      "2025-12-02 12:27:59,256: ### Folder structure created at /home/sngrj0hn/GitHub/neuralhydrology/operational_harz/gefs_10d_sample/runs/development_run_mdn_0212_122759\n",
      "2025-12-02 12:27:59,256: ### Run configurations for development_run_mdn\n",
      "2025-12-02 12:27:59,256: experiment_name: development_run_mdn\n",
      "2025-12-02 12:27:59,256: run_dir: /home/sngrj0hn/GitHub/neuralhydrology/operational_harz/gefs_10d_sample/runs/development_run_mdn_0212_122759\n",
      "2025-12-02 12:27:59,256: train_basin_file: basins.txt\n",
      "2025-12-02 12:27:59,257: validation_basin_file: basins.txt\n",
      "2025-12-02 12:27:59,257: test_basin_file: basins.txt\n",
      "2025-12-02 12:27:59,257: train_start_date: 2020-10-01 00:00:00\n",
      "2025-12-02 12:27:59,257: train_end_date: 2023-09-30 00:00:00\n",
      "2025-12-02 12:27:59,257: validation_start_date: 2023-10-01 00:00:00\n",
      "2025-12-02 12:27:59,257: validation_end_date: 2024-01-31 00:00:00\n",
      "2025-12-02 12:27:59,258: test_start_date: 2024-02-01 00:00:00\n",
      "2025-12-02 12:27:59,258: test_end_date: 2024-04-30 00:00:00\n",
      "2025-12-02 12:27:59,258: per_basin_train_periods_file: None\n",
      "2025-12-02 12:27:59,258: per_basin_validation_periods_file: None\n",
      "2025-12-02 12:27:59,258: per_basin_test_periods_file: None\n",
      "2025-12-02 12:27:59,259: seed: 32\n",
      "2025-12-02 12:27:59,259: device: cuda:0\n",
      "2025-12-02 12:27:59,259: validate_every: 4\n",
      "2025-12-02 12:27:59,259: validate_n_random_basins: 4\n",
      "2025-12-02 12:27:59,259: cache_validation_data: True\n",
      "2025-12-02 12:27:59,259: metrics: ['NSE', 'KGE', 'Alpha-NSE', 'Beta-NSE']\n",
      "2025-12-02 12:27:59,260: model: handoff_forecast_lstm\n",
      "2025-12-02 12:27:59,260: checkpoint_path: None\n",
      "2025-12-02 12:27:59,260: head: cmal\n",
      "2025-12-02 12:27:59,260: n_distributions: 3\n",
      "2025-12-02 12:27:59,260: n_samples: 100\n",
      "2025-12-02 12:27:59,260: negative_sample_handling: clip\n",
      "2025-12-02 12:27:59,261: dynamics_embedding: {'type': 'fc', 'hiddens': 64, 'activation': 'tanh', 'dropout': 0.0}\n",
      "2025-12-02 12:27:59,261: shared_mtslstm: False\n",
      "2025-12-02 12:27:59,261: transfer_mtslstm_states: {'h': 'identity', 'c': 'identity'}\n",
      "2025-12-02 12:27:59,261: ode_method: euler\n",
      "2025-12-02 12:27:59,261: ode_num_unfolds: 4\n",
      "2025-12-02 12:27:59,261: ode_random_freq_lower_bound: 6D\n",
      "2025-12-02 12:27:59,262: transformer_nlayers: 4\n",
      "2025-12-02 12:27:59,262: transformer_positional_encoding_type: sum\n",
      "2025-12-02 12:27:59,262: transformer_dim_feedforward: 32\n",
      "2025-12-02 12:27:59,262: transformer_positional_dropout: 0.0\n",
      "2025-12-02 12:27:59,262: transformer_dropout: 0\n",
      "2025-12-02 12:27:59,262: transformer_nheads: 4\n",
      "2025-12-02 12:27:59,263: hidden_size: 128\n",
      "2025-12-02 12:27:59,263: initial_forget_bias: 3\n",
      "2025-12-02 12:27:59,263: output_dropout: 0.4\n",
      "2025-12-02 12:27:59,263: optimizer: Adam\n",
      "2025-12-02 12:27:59,263: loss: CMALLoss\n",
      "2025-12-02 12:27:59,263: regularization: None\n",
      "2025-12-02 12:27:59,263: learning_rate: {0: 0.001, 40: 0.0005, 60: 0.0001}\n",
      "2025-12-02 12:27:59,264: batch_size: 256\n",
      "2025-12-02 12:27:59,264: epochs: 80\n",
      "2025-12-02 12:27:59,264: target_noise_std: 0.005\n",
      "2025-12-02 12:27:59,264: clip_gradient_norm: 1\n",
      "2025-12-02 12:27:59,264: predict_last_n: 240\n",
      "2025-12-02 12:27:59,264: seq_length: 960\n",
      "2025-12-02 12:27:59,264: forecast_seq_length: 240\n",
      "2025-12-02 12:27:59,264: forecast_offset: 0\n",
      "2025-12-02 12:27:59,265: num_workers: 0\n",
      "2025-12-02 12:27:59,265: log_interval: 5\n",
      "2025-12-02 12:27:59,265: log_tensorboard: True\n",
      "2025-12-02 12:27:59,265: log_n_figures: 2\n",
      "2025-12-02 12:27:59,265: save_weights_every: 1\n",
      "2025-12-02 12:27:59,265: save_validation_results: False\n",
      "2025-12-02 12:27:59,265: dataset: online_forecast\n",
      "2025-12-02 12:27:59,265: data_dir: ../../data/harz\n",
      "2025-12-02 12:27:59,265: train_dir: /home/sngrj0hn/GitHub/neuralhydrology/operational_harz/gefs_10d_sample/runs/development_run_mdn_0212_122759/train_data\n",
      "2025-12-02 12:27:59,266: time_series_data_sub_dir: timeseries\n",
      "2025-12-02 12:27:59,266: save_train_data: False\n",
      "2025-12-02 12:27:59,266: train_data_file: runs/train_data.zarr\n",
      "2025-12-02 12:27:59,266: dynamic_inputs: ['maximum_temperature_2m_q25', 'maximum_temperature_2m_q50', 'maximum_temperature_2m_q75', 'minimum_temperature_2m_q25', 'minimum_temperature_2m_q50', 'minimum_temperature_2m_q75', 'precipitation_surface_q25', 'precipitation_surface_q50', 'precipitation_surface_q75', 'relative_humidity_2m_q25', 'relative_humidity_2m_q50', 'relative_humidity_2m_q75', 'temperature_2m_q25', 'temperature_2m_q50', 'temperature_2m_q75', 'temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'precipitation', 'rain', 'snowfall', 'surface_pressure', 'et0_fao_evapotranspiration', 'wind_direction_10m', 'soil_moisture_0_to_7cm', 'soil_moisture_7_to_28cm', 'soil_moisture_28_to_100cm', 'shortwave_radiation']\n",
      "2025-12-02 12:27:59,266: forecast_inputs: ['maximum_temperature_2m_q25', 'maximum_temperature_2m_q50', 'maximum_temperature_2m_q75', 'minimum_temperature_2m_q25', 'minimum_temperature_2m_q50', 'minimum_temperature_2m_q75', 'precipitation_surface_q25', 'precipitation_surface_q50', 'precipitation_surface_q75', 'relative_humidity_2m_q25', 'relative_humidity_2m_q50', 'relative_humidity_2m_q75', 'temperature_2m_q25', 'temperature_2m_q50', 'temperature_2m_q75']\n",
      "2025-12-02 12:27:59,266: hindcast_inputs: ['temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'precipitation', 'rain', 'snowfall', 'surface_pressure', 'et0_fao_evapotranspiration', 'wind_direction_10m', 'soil_moisture_0_to_7cm', 'soil_moisture_7_to_28cm', 'soil_moisture_28_to_100cm', 'shortwave_radiation']\n",
      "2025-12-02 12:27:59,266: state_handoff_network: {'type': 'fc', 'hiddens': 64, 'activation': 'tanh', 'dropout': 0.0}\n",
      "2025-12-02 12:27:59,266: target_variables: ['discharge_vol']\n",
      "2025-12-02 12:27:59,267: clip_targets_to_zero: ['discharge_vol']\n",
      "2025-12-02 12:27:59,267: static_attributes: None\n",
      "2025-12-02 12:27:59,267: additional_feature_files: None\n",
      "2025-12-02 12:27:59,267: evolving_attributes: None\n",
      "2025-12-02 12:27:59,268: use_basin_id_encoding: False\n",
      "2025-12-02 12:27:59,268: number_of_basins: 5\n",
      "2025-12-02 12:27:59,268: img_log_dir: /home/sngrj0hn/GitHub/neuralhydrology/operational_harz/gefs_10d_sample/runs/development_run_mdn_0212_122759/img_log\n",
      "2025-12-02 12:27:59,265: data_dir: ../../data/harz\n",
      "2025-12-02 12:27:59,265: train_dir: /home/sngrj0hn/GitHub/neuralhydrology/operational_harz/gefs_10d_sample/runs/development_run_mdn_0212_122759/train_data\n",
      "2025-12-02 12:27:59,266: time_series_data_sub_dir: timeseries\n",
      "2025-12-02 12:27:59,266: save_train_data: False\n",
      "2025-12-02 12:27:59,266: train_data_file: runs/train_data.zarr\n",
      "2025-12-02 12:27:59,266: dynamic_inputs: ['maximum_temperature_2m_q25', 'maximum_temperature_2m_q50', 'maximum_temperature_2m_q75', 'minimum_temperature_2m_q25', 'minimum_temperature_2m_q50', 'minimum_temperature_2m_q75', 'precipitation_surface_q25', 'precipitation_surface_q50', 'precipitation_surface_q75', 'relative_humidity_2m_q25', 'relative_humidity_2m_q50', 'relative_humidity_2m_q75', 'temperature_2m_q25', 'temperature_2m_q50', 'temperature_2m_q75', 'temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'precipitation', 'rain', 'snowfall', 'surface_pressure', 'et0_fao_evapotranspiration', 'wind_direction_10m', 'soil_moisture_0_to_7cm', 'soil_moisture_7_to_28cm', 'soil_moisture_28_to_100cm', 'shortwave_radiation']\n",
      "2025-12-02 12:27:59,266: forecast_inputs: ['maximum_temperature_2m_q25', 'maximum_temperature_2m_q50', 'maximum_temperature_2m_q75', 'minimum_temperature_2m_q25', 'minimum_temperature_2m_q50', 'minimum_temperature_2m_q75', 'precipitation_surface_q25', 'precipitation_surface_q50', 'precipitation_surface_q75', 'relative_humidity_2m_q25', 'relative_humidity_2m_q50', 'relative_humidity_2m_q75', 'temperature_2m_q25', 'temperature_2m_q50', 'temperature_2m_q75']\n",
      "2025-12-02 12:27:59,266: hindcast_inputs: ['temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'precipitation', 'rain', 'snowfall', 'surface_pressure', 'et0_fao_evapotranspiration', 'wind_direction_10m', 'soil_moisture_0_to_7cm', 'soil_moisture_7_to_28cm', 'soil_moisture_28_to_100cm', 'shortwave_radiation']\n",
      "2025-12-02 12:27:59,266: state_handoff_network: {'type': 'fc', 'hiddens': 64, 'activation': 'tanh', 'dropout': 0.0}\n",
      "2025-12-02 12:27:59,266: target_variables: ['discharge_vol']\n",
      "2025-12-02 12:27:59,267: clip_targets_to_zero: ['discharge_vol']\n",
      "2025-12-02 12:27:59,267: static_attributes: None\n",
      "2025-12-02 12:27:59,267: additional_feature_files: None\n",
      "2025-12-02 12:27:59,267: evolving_attributes: None\n",
      "2025-12-02 12:27:59,268: use_basin_id_encoding: False\n",
      "2025-12-02 12:27:59,268: number_of_basins: 5\n",
      "2025-12-02 12:27:59,268: img_log_dir: /home/sngrj0hn/GitHub/neuralhydrology/operational_harz/gefs_10d_sample/runs/development_run_mdn_0212_122759/img_log\n",
      "2025-12-02 12:27:59,269: ### Device cuda:0 will be used for training\n",
      "2025-12-02 12:27:59,269: Loading cached dataset from runs/train_data.zarr\n",
      "2025-12-02 12:27:59,269: ### Device cuda:0 will be used for training\n",
      "2025-12-02 12:27:59,269: Loading cached dataset from runs/train_data.zarr\n",
      "2025-12-02 12:27:59,285: Validating data availability against configured time periods...\n",
      "2025-12-02 12:27:59,286:   Historical coverage: 2020-09-01 00:00:00 to 2024-04-30 23:00:00\n",
      "2025-12-02 12:27:59,286:   Forecast coverage: 2020-10-01 00:00:00 to 2025-12-01 00:00:00\n",
      "2025-12-02 12:27:59,286:   Checking train period: 2020-10-01 to 2023-09-30\n",
      "2025-12-02 12:27:59,286:   Checking validation period: 2023-10-01 to 2024-01-31\n",
      "2025-12-02 12:27:59,287:   Checking test period: 2024-02-01 to 2024-04-30\n",
      "2025-12-02 12:27:59,285: Validating data availability against configured time periods...\n",
      "2025-12-02 12:27:59,286:   Historical coverage: 2020-09-01 00:00:00 to 2024-04-30 23:00:00\n",
      "2025-12-02 12:27:59,286:   Forecast coverage: 2020-10-01 00:00:00 to 2025-12-01 00:00:00\n",
      "2025-12-02 12:27:59,286:   Checking train period: 2020-10-01 to 2023-09-30\n",
      "2025-12-02 12:27:59,286:   Checking validation period: 2023-10-01 to 2024-01-31\n",
      "2025-12-02 12:27:59,287:   Checking test period: 2024-02-01 to 2024-04-30\n",
      "2025-12-02 12:27:59,287: ✅ Data availability validation passed\n",
      "2025-12-02 12:27:59,287: ✅ Data availability validation passed\n",
      "2025-12-02 12:27:59,346: Create lookup table and convert to pytorch tensor\n",
      "2025-12-02 12:27:59,346: Create lookup table and convert to pytorch tensor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sngrj0hn/anaconda3/envs/neuralhydrology/lib/python3.12/site-packages/zarr/codecs/vlen_utf8.py:44: UserWarning: The codec `vlen-utf8` is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  return cls(**configuration_parsed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.29it/s]\n",
      "\n",
      "# Epoch 1:   5%|▍         | 1/22 [00:00<00:03,  5.58it/s, Loss: 298.4598]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sngrj0hn/GitHub/neuralhydrology/neuralhydrology/datasetzoo/onlineforecastdataset.py:793: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647329220/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  sample[f'x_h{freq_suffix}'] = torch.from_numpy(x_h)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Epoch 1: 100%|██████████| 22/22 [00:01<00:00, 18.59it/s, Loss: 127.3802]\n",
      "2025-12-02 12:28:03,427: Epoch 1 average loss: avg_loss: 170.45217, avg_total_loss: 170.45217\n",
      "# Epoch 1: 100%|██████████| 22/22 [00:01<00:00, 18.59it/s, Loss: 127.3802]\n",
      "2025-12-02 12:28:03,427: Epoch 1 average loss: avg_loss: 170.45217, avg_total_loss: 170.45217\n",
      "# Epoch 2: 100%|██████████| 22/22 [00:01<00:00, 21.06it/s, Loss: 72.4138] \n",
      "2025-12-02 12:28:04,476: Epoch 2 average loss: avg_loss: 95.10378, avg_total_loss: 95.10378\n",
      "# Epoch 2: 100%|██████████| 22/22 [00:01<00:00, 21.06it/s, Loss: 72.4138]\n",
      "2025-12-02 12:28:04,476: Epoch 2 average loss: avg_loss: 95.10378, avg_total_loss: 95.10378\n",
      "# Epoch 3: 100%|██████████| 22/22 [00:01<00:00, 21.31it/s, Loss: 62.5879]\n",
      "2025-12-02 12:28:05,514: Epoch 3 average loss: avg_loss: 66.11217, avg_total_loss: 66.11217\n",
      "# Epoch 3: 100%|██████████| 22/22 [00:01<00:00, 21.31it/s, Loss: 62.5879]\n",
      "2025-12-02 12:28:05,514: Epoch 3 average loss: avg_loss: 66.11217, avg_total_loss: 66.11217\n",
      "# Epoch 4: 100%|██████████| 22/22 [00:01<00:00, 21.12it/s, Loss: 43.2441]\n",
      "2025-12-02 12:28:06,561: Epoch 4 average loss: avg_loss: 43.85739, avg_total_loss: 43.85739\n",
      "# Validation:   0%|          | 0/4 [00:00<?, ?it/s]2025-12-02 12:28:06,566: Loading cached dataset from runs/train_data.zarr\n",
      "# Epoch 4: 100%|██████████| 22/22 [00:01<00:00, 21.12it/s, Loss: 43.2441]\n",
      "2025-12-02 12:28:06,561: Epoch 4 average loss: avg_loss: 43.85739, avg_total_loss: 43.85739\n",
      "# Validation:   0%|          | 0/4 [00:00<?, ?it/s]2025-12-02 12:28:06,566: Loading cached dataset from runs/train_data.zarr\n",
      "2025-12-02 12:28:06,582: Validating data availability against configured time periods...\n",
      "2025-12-02 12:28:06,582:   Historical coverage: 2020-09-01 00:00:00 to 2024-04-30 23:00:00\n",
      "2025-12-02 12:28:06,583:   Forecast coverage: 2020-10-01 00:00:00 to 2025-12-01 00:00:00\n",
      "2025-12-02 12:28:06,583:   Checking train period: 2020-10-01 to 2023-09-30\n",
      "2025-12-02 12:28:06,583:   Checking validation period: 2023-10-01 to 2024-01-31\n",
      "2025-12-02 12:28:06,583:   Checking test period: 2024-02-01 to 2024-04-30\n",
      "2025-12-02 12:28:06,583: ✅ Data availability validation passed\n",
      "2025-12-02 12:28:06,582: Validating data availability against configured time periods...\n",
      "2025-12-02 12:28:06,582:   Historical coverage: 2020-09-01 00:00:00 to 2024-04-30 23:00:00\n",
      "2025-12-02 12:28:06,583:   Forecast coverage: 2020-10-01 00:00:00 to 2025-12-01 00:00:00\n",
      "2025-12-02 12:28:06,583:   Checking train period: 2020-10-01 to 2023-09-30\n",
      "2025-12-02 12:28:06,583:   Checking validation period: 2023-10-01 to 2024-01-31\n",
      "2025-12-02 12:28:06,583:   Checking test period: 2024-02-01 to 2024-04-30\n",
      "2025-12-02 12:28:06,583: ✅ Data availability validation passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sngrj0hn/anaconda3/envs/neuralhydrology/lib/python3.12/site-packages/zarr/codecs/vlen_utf8.py:44: UserWarning: The codec `vlen-utf8` is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  return cls(**configuration_parsed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation:  25%|██▌       | 1/4 [00:02<00:06,  2.09s/it]2025-12-02 12:28:08,659: Loading cached dataset from runs/train_data.zarr\n",
      "# Validation:  25%|██▌       | 1/4 [00:02<00:06,  2.09s/it]2025-12-02 12:28:08,659: Loading cached dataset from runs/train_data.zarr\n",
      "2025-12-02 12:28:08,676: Validating data availability against configured time periods...\n",
      "2025-12-02 12:28:08,676:   Historical coverage: 2020-09-01 00:00:00 to 2024-04-30 23:00:00\n",
      "2025-12-02 12:28:08,676:   Forecast coverage: 2020-10-01 00:00:00 to 2025-12-01 00:00:00\n",
      "2025-12-02 12:28:08,676:   Checking train period: 2020-10-01 to 2023-09-30\n",
      "2025-12-02 12:28:08,676:   Checking validation period: 2023-10-01 to 2024-01-31\n",
      "2025-12-02 12:28:08,676:   Checking test period: 2024-02-01 to 2024-04-30\n",
      "2025-12-02 12:28:08,677: ✅ Data availability validation passed\n",
      "2025-12-02 12:28:08,676: Validating data availability against configured time periods...\n",
      "2025-12-02 12:28:08,676:   Historical coverage: 2020-09-01 00:00:00 to 2024-04-30 23:00:00\n",
      "2025-12-02 12:28:08,676:   Forecast coverage: 2020-10-01 00:00:00 to 2025-12-01 00:00:00\n",
      "2025-12-02 12:28:08,676:   Checking train period: 2020-10-01 to 2023-09-30\n",
      "2025-12-02 12:28:08,676:   Checking validation period: 2023-10-01 to 2024-01-31\n",
      "2025-12-02 12:28:08,676:   Checking test period: 2024-02-01 to 2024-04-30\n",
      "2025-12-02 12:28:08,677: ✅ Data availability validation passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sngrj0hn/anaconda3/envs/neuralhydrology/lib/python3.12/site-packages/zarr/codecs/vlen_utf8.py:44: UserWarning: The codec `vlen-utf8` is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  return cls(**configuration_parsed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Validation:  50%|█████     | 2/4 [00:04<00:04,  2.03s/it]2025-12-02 12:28:10,641: Loading cached dataset from runs/train_data.zarr\n",
      "2025-12-02 12:28:10,641: Loading cached dataset from runs/train_data.zarr\n",
      "2025-12-02 12:28:10,657: Validating data availability against configured time periods...\n",
      "2025-12-02 12:28:10,657:   Historical coverage: 2020-09-01 00:00:00 to 2024-04-30 23:00:00\n",
      "2025-12-02 12:28:10,657:   Forecast coverage: 2020-10-01 00:00:00 to 2025-12-01 00:00:00\n",
      "2025-12-02 12:28:10,658:   Checking train period: 2020-10-01 to 2023-09-30\n",
      "2025-12-02 12:28:10,658:   Checking validation period: 2023-10-01 to 2024-01-31\n",
      "2025-12-02 12:28:10,658:   Checking test period: 2024-02-01 to 2024-04-30\n",
      "2025-12-02 12:28:10,658: ✅ Data availability validation passed\n",
      "2025-12-02 12:28:10,657: Validating data availability against configured time periods...\n",
      "2025-12-02 12:28:10,657:   Historical coverage: 2020-09-01 00:00:00 to 2024-04-30 23:00:00\n",
      "2025-12-02 12:28:10,657:   Forecast coverage: 2020-10-01 00:00:00 to 2025-12-01 00:00:00\n",
      "2025-12-02 12:28:10,658:   Checking train period: 2020-10-01 to 2023-09-30\n",
      "2025-12-02 12:28:10,658:   Checking validation period: 2023-10-01 to 2024-01-31\n",
      "2025-12-02 12:28:10,658:   Checking test period: 2024-02-01 to 2024-04-30\n",
      "2025-12-02 12:28:10,658: ✅ Data availability validation passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sngrj0hn/anaconda3/envs/neuralhydrology/lib/python3.12/site-packages/zarr/codecs/vlen_utf8.py:44: UserWarning: The codec `vlen-utf8` is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  return cls(**configuration_parsed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Validation:  75%|███████▌  | 3/4 [00:06<00:02,  2.02s/it]2025-12-02 12:28:12,658: Loading cached dataset from runs/train_data.zarr\n",
      "2025-12-02 12:28:12,658: Loading cached dataset from runs/train_data.zarr\n",
      "2025-12-02 12:28:12,676: Validating data availability against configured time periods...\n",
      "2025-12-02 12:28:12,676:   Historical coverage: 2020-09-01 00:00:00 to 2024-04-30 23:00:00\n",
      "2025-12-02 12:28:12,676:   Forecast coverage: 2020-10-01 00:00:00 to 2025-12-01 00:00:00\n",
      "2025-12-02 12:28:12,676:   Checking train period: 2020-10-01 to 2023-09-30\n",
      "2025-12-02 12:28:12,676:   Checking validation period: 2023-10-01 to 2024-01-31\n",
      "2025-12-02 12:28:12,676:   Checking test period: 2024-02-01 to 2024-04-30\n",
      "2025-12-02 12:28:12,676: ✅ Data availability validation passed\n",
      "2025-12-02 12:28:12,676: Validating data availability against configured time periods...\n",
      "2025-12-02 12:28:12,676:   Historical coverage: 2020-09-01 00:00:00 to 2024-04-30 23:00:00\n",
      "2025-12-02 12:28:12,676:   Forecast coverage: 2020-10-01 00:00:00 to 2025-12-01 00:00:00\n",
      "2025-12-02 12:28:12,676:   Checking train period: 2020-10-01 to 2023-09-30\n",
      "2025-12-02 12:28:12,676:   Checking validation period: 2023-10-01 to 2024-01-31\n",
      "2025-12-02 12:28:12,676:   Checking test period: 2024-02-01 to 2024-04-30\n",
      "2025-12-02 12:28:12,676: ✅ Data availability validation passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sngrj0hn/anaconda3/envs/neuralhydrology/lib/python3.12/site-packages/zarr/codecs/vlen_utf8.py:44: UserWarning: The codec `vlen-utf8` is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  return cls(**configuration_parsed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Validation: 100%|██████████| 4/4 [00:08<00:00,  2.03s/it]\n",
      "\n",
      "2025-12-02 12:28:15,147: Epoch 4 average validation loss: 323.74667 -- Median validation metrics: avg_loss: 323.74667, NSE: 0.14392, KGE: 0.27951, Alpha-NSE: 0.52393, Beta-NSE: -0.30557\n",
      "# Epoch 5:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:28:15,147: Epoch 4 average validation loss: 323.74667 -- Median validation metrics: avg_loss: 323.74667, NSE: 0.14392, KGE: 0.27951, Alpha-NSE: 0.52393, Beta-NSE: -0.30557\n",
      "# Epoch 5: 100%|██████████| 22/22 [00:01<00:00, 20.99it/s, Loss: 61.2099]\n",
      "2025-12-02 12:28:16,198: Epoch 5 average loss: avg_loss: 33.18799, avg_total_loss: 33.18799\n",
      "# Epoch 5: 100%|██████████| 22/22 [00:01<00:00, 20.99it/s, Loss: 61.2099]\n",
      "2025-12-02 12:28:16,198: Epoch 5 average loss: avg_loss: 33.18799, avg_total_loss: 33.18799\n",
      "# Epoch 6: 100%|██████████| 22/22 [00:01<00:00, 21.40it/s, Loss: 31.2408]\n",
      "2025-12-02 12:28:17,231: Epoch 6 average loss: avg_loss: 16.33436, avg_total_loss: 16.33436\n",
      "# Epoch 6: 100%|██████████| 22/22 [00:01<00:00, 21.40it/s, Loss: 31.2408]\n",
      "2025-12-02 12:28:17,231: Epoch 6 average loss: avg_loss: 16.33436, avg_total_loss: 16.33436\n",
      "# Epoch 7: 100%|██████████| 22/22 [00:00<00:00, 23.46it/s, Loss: 11.3113] \n",
      "2025-12-02 12:28:18,174: Epoch 7 average loss: avg_loss: 5.30530, avg_total_loss: 5.30530\n",
      "# Epoch 7: 100%|██████████| 22/22 [00:00<00:00, 23.46it/s, Loss: 11.3113] \n",
      "2025-12-02 12:28:18,174: Epoch 7 average loss: avg_loss: 5.30530, avg_total_loss: 5.30530\n",
      "# Epoch 8: 100%|██████████| 22/22 [00:01<00:00, 21.20it/s, Loss: -0.5207] \n",
      "2025-12-02 12:28:19,216: Epoch 8 average loss: avg_loss: -0.25840, avg_total_loss: -0.25840\n",
      "# Epoch 8: 100%|██████████| 22/22 [00:01<00:00, 21.20it/s, Loss: -0.5207] \n",
      "2025-12-02 12:28:19,216: Epoch 8 average loss: avg_loss: -0.25840, avg_total_loss: -0.25840\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation:  75%|███████▌  | 3/4 [00:05<00:01,  1.77s/it]2025-12-02 12:28:24,512: Loading cached dataset from runs/train_data.zarr\n",
      "# Validation:  75%|███████▌  | 3/4 [00:05<00:01,  1.77s/it]2025-12-02 12:28:24,512: Loading cached dataset from runs/train_data.zarr\n",
      "2025-12-02 12:28:24,528: Validating data availability against configured time periods...\n",
      "2025-12-02 12:28:24,529:   Historical coverage: 2020-09-01 00:00:00 to 2024-04-30 23:00:00\n",
      "2025-12-02 12:28:24,529:   Forecast coverage: 2020-10-01 00:00:00 to 2025-12-01 00:00:00\n",
      "2025-12-02 12:28:24,529:   Checking train period: 2020-10-01 to 2023-09-30\n",
      "2025-12-02 12:28:24,529:   Checking validation period: 2023-10-01 to 2024-01-31\n",
      "2025-12-02 12:28:24,529:   Checking test period: 2024-02-01 to 2024-04-30\n",
      "2025-12-02 12:28:24,529: ✅ Data availability validation passed\n",
      "2025-12-02 12:28:24,528: Validating data availability against configured time periods...\n",
      "2025-12-02 12:28:24,529:   Historical coverage: 2020-09-01 00:00:00 to 2024-04-30 23:00:00\n",
      "2025-12-02 12:28:24,529:   Forecast coverage: 2020-10-01 00:00:00 to 2025-12-01 00:00:00\n",
      "2025-12-02 12:28:24,529:   Checking train period: 2020-10-01 to 2023-09-30\n",
      "2025-12-02 12:28:24,529:   Checking validation period: 2023-10-01 to 2024-01-31\n",
      "2025-12-02 12:28:24,529:   Checking test period: 2024-02-01 to 2024-04-30\n",
      "2025-12-02 12:28:24,529: ✅ Data availability validation passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sngrj0hn/anaconda3/envs/neuralhydrology/lib/python3.12/site-packages/zarr/codecs/vlen_utf8.py:44: UserWarning: The codec `vlen-utf8` is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  return cls(**configuration_parsed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Validation: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]\n",
      "\n",
      "2025-12-02 12:28:26,924: Epoch 8 average validation loss: 347.69742 -- Median validation metrics: avg_loss: 347.69742, NSE: 0.22480, KGE: 0.37555, Alpha-NSE: 0.60978, Beta-NSE: -0.13922\n",
      "# Epoch 9:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:28:26,924: Epoch 8 average validation loss: 347.69742 -- Median validation metrics: avg_loss: 347.69742, NSE: 0.22480, KGE: 0.37555, Alpha-NSE: 0.60978, Beta-NSE: -0.13922\n",
      "# Epoch 9: 100%|██████████| 22/22 [00:01<00:00, 20.47it/s, Loss: 11.7942] \n",
      "2025-12-02 12:28:28,000: Epoch 9 average loss: avg_loss: -12.04331, avg_total_loss: -12.04331\n",
      "# Epoch 9: 100%|██████████| 22/22 [00:01<00:00, 20.47it/s, Loss: 11.7942] \n",
      "2025-12-02 12:28:28,000: Epoch 9 average loss: avg_loss: -12.04331, avg_total_loss: -12.04331\n",
      "# Epoch 10: 100%|██████████| 22/22 [00:00<00:00, 23.83it/s, Loss: -35.5640]\n",
      "2025-12-02 12:28:28,929: Epoch 10 average loss: avg_loss: -24.09969, avg_total_loss: -24.09969\n",
      "# Epoch 10: 100%|██████████| 22/22 [00:00<00:00, 23.83it/s, Loss: -35.5640]\n",
      "2025-12-02 12:28:28,929: Epoch 10 average loss: avg_loss: -24.09969, avg_total_loss: -24.09969\n",
      "# Epoch 11: 100%|██████████| 22/22 [00:01<00:00, 21.33it/s, Loss: -28.9864]\n",
      "2025-12-02 12:28:29,966: Epoch 11 average loss: avg_loss: -32.61987, avg_total_loss: -32.61987\n",
      "# Epoch 11: 100%|██████████| 22/22 [00:01<00:00, 21.33it/s, Loss: -28.9864]\n",
      "2025-12-02 12:28:29,966: Epoch 11 average loss: avg_loss: -32.61987, avg_total_loss: -32.61987\n",
      "# Epoch 12: 100%|██████████| 22/22 [00:01<00:00, 20.68it/s, Loss: -38.4051]\n",
      "2025-12-02 12:28:31,035: Epoch 12 average loss: avg_loss: -39.91651, avg_total_loss: -39.91651\n",
      "# Epoch 12: 100%|██████████| 22/22 [00:01<00:00, 20.68it/s, Loss: -38.4051]\n",
      "2025-12-02 12:28:31,035: Epoch 12 average loss: avg_loss: -39.91651, avg_total_loss: -39.91651\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:07<00:00,  1.75s/it]\n",
      "\n",
      "2025-12-02 12:28:38,484: Epoch 12 average validation loss: 357.05955 -- Median validation metrics: avg_loss: 357.05955, NSE: 0.15131, KGE: 0.23711, Alpha-NSE: 0.42056, Beta-NSE: -0.37758\n",
      "# Epoch 13:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:28:38,484: Epoch 12 average validation loss: 357.05955 -- Median validation metrics: avg_loss: 357.05955, NSE: 0.15131, KGE: 0.23711, Alpha-NSE: 0.42056, Beta-NSE: -0.37758\n",
      "# Epoch 13: 100%|██████████| 22/22 [00:01<00:00, 20.33it/s, Loss: -33.0915]\n",
      "2025-12-02 12:28:39,568: Epoch 13 average loss: avg_loss: -50.31699, avg_total_loss: -50.31699\n",
      "# Epoch 13: 100%|██████████| 22/22 [00:01<00:00, 20.33it/s, Loss: -33.0915]\n",
      "2025-12-02 12:28:39,568: Epoch 13 average loss: avg_loss: -50.31699, avg_total_loss: -50.31699\n",
      "# Epoch 14: 100%|██████████| 22/22 [00:01<00:00, 20.13it/s, Loss: -79.5707]\n",
      "2025-12-02 12:28:40,665: Epoch 14 average loss: avg_loss: -63.92600, avg_total_loss: -63.92600\n",
      "# Epoch 14: 100%|██████████| 22/22 [00:01<00:00, 20.13it/s, Loss: -79.5707]\n",
      "2025-12-02 12:28:40,665: Epoch 14 average loss: avg_loss: -63.92600, avg_total_loss: -63.92600\n",
      "# Epoch 15: 100%|██████████| 22/22 [00:01<00:00, 20.87it/s, Loss: -44.6911]\n",
      "2025-12-02 12:28:41,725: Epoch 15 average loss: avg_loss: -59.56463, avg_total_loss: -59.56463\n",
      "# Epoch 15: 100%|██████████| 22/22 [00:01<00:00, 20.87it/s, Loss: -44.6911]\n",
      "2025-12-02 12:28:41,725: Epoch 15 average loss: avg_loss: -59.56463, avg_total_loss: -59.56463\n",
      "# Epoch 16: 100%|██████████| 22/22 [00:01<00:00, 21.06it/s, Loss: -58.2772]\n",
      "2025-12-02 12:28:42,774: Epoch 16 average loss: avg_loss: -71.82260, avg_total_loss: -71.82260\n",
      "# Epoch 16: 100%|██████████| 22/22 [00:01<00:00, 21.06it/s, Loss: -58.2772]\n",
      "2025-12-02 12:28:42,774: Epoch 16 average loss: avg_loss: -71.82260, avg_total_loss: -71.82260\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:07<00:00,  1.76s/it]\n",
      "\n",
      "2025-12-02 12:28:50,256: Epoch 16 average validation loss: 375.59307 -- Median validation metrics: avg_loss: 375.59307, NSE: 0.24421, KGE: 0.33556, Alpha-NSE: 0.53919, Beta-NSE: -0.22275\n",
      "# Epoch 17:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:28:50,256: Epoch 16 average validation loss: 375.59307 -- Median validation metrics: avg_loss: 375.59307, NSE: 0.24421, KGE: 0.33556, Alpha-NSE: 0.53919, Beta-NSE: -0.22275\n",
      "# Epoch 17: 100%|██████████| 22/22 [00:01<00:00, 20.31it/s, Loss: -84.4640]\n",
      "2025-12-02 12:28:51,340: Epoch 17 average loss: avg_loss: -86.04819, avg_total_loss: -86.04819\n",
      "# Epoch 17: 100%|██████████| 22/22 [00:01<00:00, 20.31it/s, Loss: -84.4640]\n",
      "2025-12-02 12:28:51,340: Epoch 17 average loss: avg_loss: -86.04819, avg_total_loss: -86.04819\n",
      "# Epoch 18: 100%|██████████| 22/22 [00:01<00:00, 20.89it/s, Loss: -85.3269] \n",
      "2025-12-02 12:28:52,399: Epoch 18 average loss: avg_loss: -85.75662, avg_total_loss: -85.75662\n",
      "# Epoch 18: 100%|██████████| 22/22 [00:01<00:00, 20.89it/s, Loss: -85.3269] \n",
      "2025-12-02 12:28:52,399: Epoch 18 average loss: avg_loss: -85.75662, avg_total_loss: -85.75662\n",
      "# Epoch 19: 100%|██████████| 22/22 [00:00<00:00, 22.73it/s, Loss: -93.4355] \n",
      "2025-12-02 12:28:53,371: Epoch 19 average loss: avg_loss: -96.81732, avg_total_loss: -96.81732\n",
      "# Epoch 19: 100%|██████████| 22/22 [00:00<00:00, 22.73it/s, Loss: -93.4355] \n",
      "2025-12-02 12:28:53,371: Epoch 19 average loss: avg_loss: -96.81732, avg_total_loss: -96.81732\n",
      "# Epoch 20: 100%|██████████| 22/22 [00:01<00:00, 21.02it/s, Loss: -118.7221]\n",
      "2025-12-02 12:28:54,423: Epoch 20 average loss: avg_loss: -104.64596, avg_total_loss: -104.64596\n",
      "# Epoch 20: 100%|██████████| 22/22 [00:01<00:00, 21.02it/s, Loss: -118.7221]\n",
      "2025-12-02 12:28:54,423: Epoch 20 average loss: avg_loss: -104.64596, avg_total_loss: -104.64596\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:06<00:00,  1.75s/it]\n",
      "\n",
      "2025-12-02 12:29:01,887: Epoch 20 average validation loss: 394.20250 -- Median validation metrics: avg_loss: 394.20250, NSE: 0.26400, KGE: 0.43948, Alpha-NSE: 0.65148, Beta-NSE: -0.28680\n",
      "# Epoch 21:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:29:01,887: Epoch 20 average validation loss: 394.20250 -- Median validation metrics: avg_loss: 394.20250, NSE: 0.26400, KGE: 0.43948, Alpha-NSE: 0.65148, Beta-NSE: -0.28680\n",
      "# Epoch 21: 100%|██████████| 22/22 [00:01<00:00, 20.77it/s, Loss: -93.0031] \n",
      "2025-12-02 12:29:02,948: Epoch 21 average loss: avg_loss: -102.36442, avg_total_loss: -102.36442\n",
      "# Epoch 21: 100%|██████████| 22/22 [00:01<00:00, 20.77it/s, Loss: -93.0031]\n",
      "2025-12-02 12:29:02,948: Epoch 21 average loss: avg_loss: -102.36442, avg_total_loss: -102.36442\n",
      "# Epoch 22: 100%|██████████| 22/22 [00:01<00:00, 20.37it/s, Loss: -124.1338]\n",
      "2025-12-02 12:29:04,033: Epoch 22 average loss: avg_loss: -111.50344, avg_total_loss: -111.50344\n",
      "# Epoch 22: 100%|██████████| 22/22 [00:01<00:00, 20.37it/s, Loss: -124.1338]\n",
      "2025-12-02 12:29:04,033: Epoch 22 average loss: avg_loss: -111.50344, avg_total_loss: -111.50344\n",
      "# Epoch 23: 100%|██████████| 22/22 [00:01<00:00, 20.43it/s, Loss: -109.6753]\n",
      "2025-12-02 12:29:05,115: Epoch 23 average loss: avg_loss: -122.28371, avg_total_loss: -122.28371\n",
      "# Epoch 23: 100%|██████████| 22/22 [00:01<00:00, 20.43it/s, Loss: -109.6753]\n",
      "2025-12-02 12:29:05,115: Epoch 23 average loss: avg_loss: -122.28371, avg_total_loss: -122.28371\n",
      "# Epoch 24: 100%|██████████| 22/22 [00:00<00:00, 22.67it/s, Loss: -87.4190] \n",
      "2025-12-02 12:29:06,090: Epoch 24 average loss: avg_loss: -124.55045, avg_total_loss: -124.55045\n",
      "# Epoch 24: 100%|██████████| 22/22 [00:00<00:00, 22.67it/s, Loss: -87.4190] \n",
      "2025-12-02 12:29:06,090: Epoch 24 average loss: avg_loss: -124.55045, avg_total_loss: -124.55045\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:07<00:00,  1.77s/it]\n",
      "\n",
      "2025-12-02 12:29:13,717: Epoch 24 average validation loss: 414.58913 -- Median validation metrics: avg_loss: 414.58913, NSE: 0.07492, KGE: 0.39503, Alpha-NSE: 0.61486, Beta-NSE: -0.39587\n",
      "# Epoch 25:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:29:13,717: Epoch 24 average validation loss: 414.58913 -- Median validation metrics: avg_loss: 414.58913, NSE: 0.07492, KGE: 0.39503, Alpha-NSE: 0.61486, Beta-NSE: -0.39587\n",
      "# Epoch 25: 100%|██████████| 22/22 [00:00<00:00, 23.04it/s, Loss: -111.9737]\n",
      "2025-12-02 12:29:14,674: Epoch 25 average loss: avg_loss: -112.37723, avg_total_loss: -112.37723\n",
      "# Epoch 25: 100%|██████████| 22/22 [00:00<00:00, 23.04it/s, Loss: -111.9737]\n",
      "2025-12-02 12:29:14,674: Epoch 25 average loss: avg_loss: -112.37723, avg_total_loss: -112.37723\n",
      "# Epoch 26: 100%|██████████| 22/22 [00:01<00:00, 20.53it/s, Loss: -145.5840]\n",
      "2025-12-02 12:29:15,751: Epoch 26 average loss: avg_loss: -128.74008, avg_total_loss: -128.74008\n",
      "# Epoch 26: 100%|██████████| 22/22 [00:01<00:00, 20.53it/s, Loss: -145.5840]\n",
      "2025-12-02 12:29:15,751: Epoch 26 average loss: avg_loss: -128.74008, avg_total_loss: -128.74008\n",
      "# Epoch 27: 100%|██████████| 22/22 [00:01<00:00, 20.65it/s, Loss: -153.7579]\n",
      "2025-12-02 12:29:16,821: Epoch 27 average loss: avg_loss: -134.72192, avg_total_loss: -134.72192\n",
      "# Epoch 27: 100%|██████████| 22/22 [00:01<00:00, 20.65it/s, Loss: -153.7579]\n",
      "2025-12-02 12:29:16,821: Epoch 27 average loss: avg_loss: -134.72192, avg_total_loss: -134.72192\n",
      "# Epoch 28: 100%|██████████| 22/22 [00:01<00:00, 20.88it/s, Loss: -135.2536]\n",
      "2025-12-02 12:29:17,881: Epoch 28 average loss: avg_loss: -143.88672, avg_total_loss: -143.88672\n",
      "# Epoch 28: 100%|██████████| 22/22 [00:01<00:00, 20.88it/s, Loss: -135.2536]\n",
      "2025-12-02 12:29:17,881: Epoch 28 average loss: avg_loss: -143.88672, avg_total_loss: -143.88672\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:07<00:00,  1.75s/it]\n",
      "\n",
      "2025-12-02 12:29:25,337: Epoch 28 average validation loss: 369.18333 -- Median validation metrics: avg_loss: 369.18333, NSE: 0.16371, KGE: 0.37427, Alpha-NSE: 0.55976, Beta-NSE: -0.27461\n",
      "# Epoch 29:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:29:25,337: Epoch 28 average validation loss: 369.18333 -- Median validation metrics: avg_loss: 369.18333, NSE: 0.16371, KGE: 0.37427, Alpha-NSE: 0.55976, Beta-NSE: -0.27461\n",
      "# Epoch 29: 100%|██████████| 22/22 [00:00<00:00, 22.74it/s, Loss: -155.6433]\n",
      "2025-12-02 12:29:26,306: Epoch 29 average loss: avg_loss: -146.60069, avg_total_loss: -146.60069\n",
      "# Epoch 29: 100%|██████████| 22/22 [00:00<00:00, 22.74it/s, Loss: -155.6433]\n",
      "2025-12-02 12:29:26,306: Epoch 29 average loss: avg_loss: -146.60069, avg_total_loss: -146.60069\n",
      "# Epoch 30: 100%|██████████| 22/22 [00:01<00:00, 20.77it/s, Loss: -145.9476]\n",
      "2025-12-02 12:29:27,370: Epoch 30 average loss: avg_loss: -144.74953, avg_total_loss: -144.74953\n",
      "# Epoch 30: 100%|██████████| 22/22 [00:01<00:00, 20.77it/s, Loss: -145.9476]\n",
      "2025-12-02 12:29:27,370: Epoch 30 average loss: avg_loss: -144.74953, avg_total_loss: -144.74953\n",
      "# Epoch 31: 100%|██████████| 22/22 [00:01<00:00, 19.75it/s, Loss: -147.4290]\n",
      "2025-12-02 12:29:28,489: Epoch 31 average loss: avg_loss: -148.91312, avg_total_loss: -148.91312\n",
      "# Epoch 31: 100%|██████████| 22/22 [00:01<00:00, 19.75it/s, Loss: -147.4290]\n",
      "2025-12-02 12:29:28,489: Epoch 31 average loss: avg_loss: -148.91312, avg_total_loss: -148.91312\n",
      "# Epoch 32: 100%|██████████| 22/22 [00:01<00:00, 20.21it/s, Loss: -162.7868]\n",
      "2025-12-02 12:29:29,583: Epoch 32 average loss: avg_loss: -152.35378, avg_total_loss: -152.35378\n",
      "# Epoch 32: 100%|██████████| 22/22 [00:01<00:00, 20.21it/s, Loss: -162.7868]\n",
      "2025-12-02 12:29:29,583: Epoch 32 average loss: avg_loss: -152.35378, avg_total_loss: -152.35378\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:06<00:00,  1.73s/it]\n",
      "\n",
      "2025-12-02 12:29:36,958: Epoch 32 average validation loss: 309.69871 -- Median validation metrics: avg_loss: 309.69871, NSE: 0.23665, KGE: 0.37262, Alpha-NSE: 0.58736, Beta-NSE: -0.25917\n",
      "# Epoch 33:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:29:36,958: Epoch 32 average validation loss: 309.69871 -- Median validation metrics: avg_loss: 309.69871, NSE: 0.23665, KGE: 0.37262, Alpha-NSE: 0.58736, Beta-NSE: -0.25917\n",
      "# Epoch 33: 100%|██████████| 22/22 [00:01<00:00, 20.31it/s, Loss: -131.7734]\n",
      "2025-12-02 12:29:38,043: Epoch 33 average loss: avg_loss: -157.27750, avg_total_loss: -157.27750\n",
      "# Epoch 33: 100%|██████████| 22/22 [00:01<00:00, 20.31it/s, Loss: -131.7734]\n",
      "2025-12-02 12:29:38,043: Epoch 33 average loss: avg_loss: -157.27750, avg_total_loss: -157.27750\n",
      "# Epoch 34: 100%|██████████| 22/22 [00:01<00:00, 20.75it/s, Loss: -176.2024]\n",
      "2025-12-02 12:29:39,109: Epoch 34 average loss: avg_loss: -156.15931, avg_total_loss: -156.15931\n",
      "# Epoch 34: 100%|██████████| 22/22 [00:01<00:00, 20.75it/s, Loss: -176.2024]\n",
      "2025-12-02 12:29:39,109: Epoch 34 average loss: avg_loss: -156.15931, avg_total_loss: -156.15931\n",
      "# Epoch 35: 100%|██████████| 22/22 [00:01<00:00, 20.52it/s, Loss: -141.2604]\n",
      "2025-12-02 12:29:40,186: Epoch 35 average loss: avg_loss: -165.97153, avg_total_loss: -165.97153\n",
      "# Epoch 35: 100%|██████████| 22/22 [00:01<00:00, 20.52it/s, Loss: -141.2604]\n",
      "2025-12-02 12:29:40,186: Epoch 35 average loss: avg_loss: -165.97153, avg_total_loss: -165.97153\n",
      "# Epoch 36: 100%|██████████| 22/22 [00:00<00:00, 23.31it/s, Loss: -177.6986]\n",
      "2025-12-02 12:29:41,134: Epoch 36 average loss: avg_loss: -163.77677, avg_total_loss: -163.77677\n",
      "# Epoch 36: 100%|██████████| 22/22 [00:00<00:00, 23.31it/s, Loss: -177.6986]\n",
      "2025-12-02 12:29:41,134: Epoch 36 average loss: avg_loss: -163.77677, avg_total_loss: -163.77677\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:07<00:00,  1.76s/it]\n",
      "\n",
      "2025-12-02 12:29:48,634: Epoch 36 average validation loss: 448.77129 -- Median validation metrics: avg_loss: 448.77129, NSE: 0.00288, KGE: 0.26613, Alpha-NSE: 0.48657, Beta-NSE: -0.41174\n",
      "# Epoch 37:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:29:48,634: Epoch 36 average validation loss: 448.77129 -- Median validation metrics: avg_loss: 448.77129, NSE: 0.00288, KGE: 0.26613, Alpha-NSE: 0.48657, Beta-NSE: -0.41174\n",
      "# Epoch 37: 100%|██████████| 22/22 [00:01<00:00, 20.62it/s, Loss: -182.8109]\n",
      "2025-12-02 12:29:49,703: Epoch 37 average loss: avg_loss: -167.08224, avg_total_loss: -167.08224\n",
      "# Epoch 37: 100%|██████████| 22/22 [00:01<00:00, 20.62it/s, Loss: -182.8109]\n",
      "2025-12-02 12:29:49,703: Epoch 37 average loss: avg_loss: -167.08224, avg_total_loss: -167.08224\n",
      "# Epoch 38: 100%|██████████| 22/22 [00:01<00:00, 20.34it/s, Loss: -150.3418]\n",
      "2025-12-02 12:29:50,789: Epoch 38 average loss: avg_loss: -170.87859, avg_total_loss: -170.87859\n",
      "# Epoch 38: 100%|██████████| 22/22 [00:01<00:00, 20.34it/s, Loss: -150.3418]\n",
      "2025-12-02 12:29:50,789: Epoch 38 average loss: avg_loss: -170.87859, avg_total_loss: -170.87859\n",
      "# Epoch 39: 100%|██████████| 22/22 [00:01<00:00, 20.41it/s, Loss: -167.4273]\n",
      "2025-12-02 12:29:51,872: Epoch 39 average loss: avg_loss: -176.13375, avg_total_loss: -176.13375\n",
      "2025-12-02 12:29:51,876: Setting learning rate to 0.0005\n",
      "# Epoch 39: 100%|██████████| 22/22 [00:01<00:00, 20.41it/s, Loss: -167.4273]\n",
      "2025-12-02 12:29:51,872: Epoch 39 average loss: avg_loss: -176.13375, avg_total_loss: -176.13375\n",
      "2025-12-02 12:29:51,876: Setting learning rate to 0.0005\n",
      "# Epoch 40: 100%|██████████| 22/22 [00:00<00:00, 22.48it/s, Loss: -188.9118]\n",
      "2025-12-02 12:29:52,856: Epoch 40 average loss: avg_loss: -180.65933, avg_total_loss: -180.65933\n",
      "# Epoch 40: 100%|██████████| 22/22 [00:00<00:00, 22.48it/s, Loss: -188.9118]\n",
      "2025-12-02 12:29:52,856: Epoch 40 average loss: avg_loss: -180.65933, avg_total_loss: -180.65933\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:07<00:00,  1.79s/it]\n",
      "\n",
      "2025-12-02 12:30:00,446: Epoch 40 average validation loss: 467.96841 -- Median validation metrics: avg_loss: 467.96841, NSE: 0.01103, KGE: 0.35544, Alpha-NSE: 0.58673, Beta-NSE: -0.38961\n",
      "# Epoch 41:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:30:00,446: Epoch 40 average validation loss: 467.96841 -- Median validation metrics: avg_loss: 467.96841, NSE: 0.01103, KGE: 0.35544, Alpha-NSE: 0.58673, Beta-NSE: -0.38961\n",
      "# Epoch 41: 100%|██████████| 22/22 [00:01<00:00, 20.82it/s, Loss: -204.1806]\n",
      "2025-12-02 12:30:01,504: Epoch 41 average loss: avg_loss: -190.31163, avg_total_loss: -190.31163\n",
      "# Epoch 41: 100%|██████████| 22/22 [00:01<00:00, 20.82it/s, Loss: -204.1806]\n",
      "2025-12-02 12:30:01,504: Epoch 41 average loss: avg_loss: -190.31163, avg_total_loss: -190.31163\n",
      "# Epoch 42: 100%|██████████| 22/22 [00:00<00:00, 23.17it/s, Loss: -224.8860]\n",
      "2025-12-02 12:30:02,459: Epoch 42 average loss: avg_loss: -198.69719, avg_total_loss: -198.69719\n",
      "# Epoch 42: 100%|██████████| 22/22 [00:00<00:00, 23.17it/s, Loss: -224.8860]\n",
      "2025-12-02 12:30:02,459: Epoch 42 average loss: avg_loss: -198.69719, avg_total_loss: -198.69719\n",
      "# Epoch 43: 100%|██████████| 22/22 [00:01<00:00, 20.26it/s, Loss: -186.7803]\n",
      "2025-12-02 12:30:03,554: Epoch 43 average loss: avg_loss: -198.80822, avg_total_loss: -198.80822\n",
      "# Epoch 43: 100%|██████████| 22/22 [00:01<00:00, 20.26it/s, Loss: -186.7803]\n",
      "2025-12-02 12:30:03,554: Epoch 43 average loss: avg_loss: -198.80822, avg_total_loss: -198.80822\n",
      "# Epoch 44: 100%|██████████| 22/22 [00:01<00:00, 20.51it/s, Loss: -228.3910]\n",
      "2025-12-02 12:30:04,632: Epoch 44 average loss: avg_loss: -201.59554, avg_total_loss: -201.59554\n",
      "# Epoch 44: 100%|██████████| 22/22 [00:01<00:00, 20.51it/s, Loss: -228.3910]\n",
      "2025-12-02 12:30:04,632: Epoch 44 average loss: avg_loss: -201.59554, avg_total_loss: -201.59554\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:06<00:00,  1.74s/it]\n",
      "\n",
      "2025-12-02 12:30:12,090: Epoch 44 average validation loss: 514.78645 -- Median validation metrics: avg_loss: 514.78645, NSE: 0.01333, KGE: 0.36061, Alpha-NSE: 0.58759, Beta-NSE: -0.36568\n",
      "# Epoch 45:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:30:12,090: Epoch 44 average validation loss: 514.78645 -- Median validation metrics: avg_loss: 514.78645, NSE: 0.01333, KGE: 0.36061, Alpha-NSE: 0.58759, Beta-NSE: -0.36568\n",
      "# Epoch 45: 100%|██████████| 22/22 [00:01<00:00, 20.05it/s, Loss: -210.9020]\n",
      "2025-12-02 12:30:13,188: Epoch 45 average loss: avg_loss: -200.87511, avg_total_loss: -200.87511\n",
      "# Epoch 45: 100%|██████████| 22/22 [00:01<00:00, 20.05it/s, Loss: -210.9020]\n",
      "2025-12-02 12:30:13,188: Epoch 45 average loss: avg_loss: -200.87511, avg_total_loss: -200.87511\n",
      "# Epoch 46: 100%|██████████| 22/22 [00:01<00:00, 19.91it/s, Loss: -177.6241]\n",
      "2025-12-02 12:30:14,298: Epoch 46 average loss: avg_loss: -206.68885, avg_total_loss: -206.68885\n",
      "# Epoch 46: 100%|██████████| 22/22 [00:01<00:00, 19.91it/s, Loss: -177.6241]\n",
      "2025-12-02 12:30:14,298: Epoch 46 average loss: avg_loss: -206.68885, avg_total_loss: -206.68885\n",
      "# Epoch 47: 100%|██████████| 22/22 [00:00<00:00, 22.89it/s, Loss: -248.8007]\n",
      "2025-12-02 12:30:15,265: Epoch 47 average loss: avg_loss: -213.63829, avg_total_loss: -213.63829\n",
      "# Epoch 47: 100%|██████████| 22/22 [00:00<00:00, 22.89it/s, Loss: -248.8007]\n",
      "2025-12-02 12:30:15,265: Epoch 47 average loss: avg_loss: -213.63829, avg_total_loss: -213.63829\n",
      "# Epoch 48: 100%|██████████| 22/22 [00:01<00:00, 19.95it/s, Loss: -236.0267]\n",
      "2025-12-02 12:30:16,372: Epoch 48 average loss: avg_loss: -213.56923, avg_total_loss: -213.56923\n",
      "# Epoch 48: 100%|██████████| 22/22 [00:01<00:00, 19.95it/s, Loss: -236.0267]\n",
      "2025-12-02 12:30:16,372: Epoch 48 average loss: avg_loss: -213.56923, avg_total_loss: -213.56923\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:07<00:00,  1.77s/it]\n",
      "\n",
      "2025-12-02 12:30:23,916: Epoch 48 average validation loss: 398.87494 -- Median validation metrics: avg_loss: 398.87494, NSE: 0.19547, KGE: 0.35590, Alpha-NSE: 0.57613, Beta-NSE: -0.34086\n",
      "# Epoch 49:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:30:23,916: Epoch 48 average validation loss: 398.87494 -- Median validation metrics: avg_loss: 398.87494, NSE: 0.19547, KGE: 0.35590, Alpha-NSE: 0.57613, Beta-NSE: -0.34086\n",
      "# Epoch 49: 100%|██████████| 22/22 [00:00<00:00, 22.67it/s, Loss: -197.7661]\n",
      "2025-12-02 12:30:24,888: Epoch 49 average loss: avg_loss: -215.45719, avg_total_loss: -215.45719\n",
      "# Epoch 49: 100%|██████████| 22/22 [00:00<00:00, 22.67it/s, Loss: -197.7661]\n",
      "2025-12-02 12:30:24,888: Epoch 49 average loss: avg_loss: -215.45719, avg_total_loss: -215.45719\n",
      "# Epoch 50: 100%|██████████| 22/22 [00:01<00:00, 20.24it/s, Loss: -218.7977]\n",
      "2025-12-02 12:30:25,981: Epoch 50 average loss: avg_loss: -216.83572, avg_total_loss: -216.83572\n",
      "# Epoch 50: 100%|██████████| 22/22 [00:01<00:00, 20.24it/s, Loss: -218.7977]\n",
      "2025-12-02 12:30:25,981: Epoch 50 average loss: avg_loss: -216.83572, avg_total_loss: -216.83572\n",
      "# Epoch 51: 100%|██████████| 22/22 [00:01<00:00, 19.91it/s, Loss: -219.0338]\n",
      "2025-12-02 12:30:27,091: Epoch 51 average loss: avg_loss: -215.03492, avg_total_loss: -215.03492\n",
      "# Epoch 51: 100%|██████████| 22/22 [00:01<00:00, 19.91it/s, Loss: -219.0338]\n",
      "2025-12-02 12:30:27,091: Epoch 51 average loss: avg_loss: -215.03492, avg_total_loss: -215.03492\n",
      "# Epoch 52: 100%|██████████| 22/22 [00:00<00:00, 22.48it/s, Loss: -226.2058]\n",
      "2025-12-02 12:30:28,078: Epoch 52 average loss: avg_loss: -218.95960, avg_total_loss: -218.95960\n",
      "# Epoch 52: 100%|██████████| 22/22 [00:00<00:00, 22.48it/s, Loss: -226.2058]\n",
      "2025-12-02 12:30:28,078: Epoch 52 average loss: avg_loss: -218.95960, avg_total_loss: -218.95960\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:07<00:00,  1.79s/it]\n",
      "\n",
      "2025-12-02 12:30:35,661: Epoch 52 average validation loss: 573.69353 -- Median validation metrics: avg_loss: 573.69353, NSE: -0.00823, KGE: 0.32954, Alpha-NSE: 0.58430, Beta-NSE: -0.37032\n",
      "# Epoch 53:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:30:35,661: Epoch 52 average validation loss: 573.69353 -- Median validation metrics: avg_loss: 573.69353, NSE: -0.00823, KGE: 0.32954, Alpha-NSE: 0.58430, Beta-NSE: -0.37032\n",
      "# Epoch 53: 100%|██████████| 22/22 [00:01<00:00, 20.34it/s, Loss: -185.9889]\n",
      "2025-12-02 12:30:36,745: Epoch 53 average loss: avg_loss: -220.76092, avg_total_loss: -220.76092\n",
      "# Epoch 53: 100%|██████████| 22/22 [00:01<00:00, 20.34it/s, Loss: -185.9889]\n",
      "2025-12-02 12:30:36,745: Epoch 53 average loss: avg_loss: -220.76092, avg_total_loss: -220.76092\n",
      "# Epoch 54: 100%|██████████| 22/22 [00:00<00:00, 22.60it/s, Loss: -229.3432]\n",
      "2025-12-02 12:30:37,723: Epoch 54 average loss: avg_loss: -220.13299, avg_total_loss: -220.13299\n",
      "# Epoch 54: 100%|██████████| 22/22 [00:00<00:00, 22.60it/s, Loss: -229.3432]\n",
      "2025-12-02 12:30:37,723: Epoch 54 average loss: avg_loss: -220.13299, avg_total_loss: -220.13299\n",
      "# Epoch 55: 100%|██████████| 22/22 [00:01<00:00, 20.43it/s, Loss: -208.6637]\n",
      "2025-12-02 12:30:38,805: Epoch 55 average loss: avg_loss: -223.16339, avg_total_loss: -223.16339\n",
      "# Epoch 55: 100%|██████████| 22/22 [00:01<00:00, 20.43it/s, Loss: -208.6637]\n",
      "2025-12-02 12:30:38,805: Epoch 55 average loss: avg_loss: -223.16339, avg_total_loss: -223.16339\n",
      "# Epoch 56: 100%|██████████| 22/22 [00:01<00:00, 20.17it/s, Loss: -250.4442]\n",
      "2025-12-02 12:30:39,901: Epoch 56 average loss: avg_loss: -225.13249, avg_total_loss: -225.13249\n",
      "# Epoch 56: 100%|██████████| 22/22 [00:01<00:00, 20.17it/s, Loss: -250.4442]\n",
      "2025-12-02 12:30:39,901: Epoch 56 average loss: avg_loss: -225.13249, avg_total_loss: -225.13249\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:07<00:00,  1.76s/it]\n",
      "\n",
      "2025-12-02 12:30:47,405: Epoch 56 average validation loss: 601.28970 -- Median validation metrics: avg_loss: 601.28970, NSE: -0.03351, KGE: 0.34865, Alpha-NSE: 0.60399, Beta-NSE: -0.41943\n",
      "# Epoch 57:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:30:47,405: Epoch 56 average validation loss: 601.28970 -- Median validation metrics: avg_loss: 601.28970, NSE: -0.03351, KGE: 0.34865, Alpha-NSE: 0.60399, Beta-NSE: -0.41943\n",
      "# Epoch 57: 100%|██████████| 22/22 [00:00<00:00, 22.92it/s, Loss: -230.1134]\n",
      "2025-12-02 12:30:48,367: Epoch 57 average loss: avg_loss: -223.51692, avg_total_loss: -223.51692\n",
      "# Epoch 57: 100%|██████████| 22/22 [00:00<00:00, 22.92it/s, Loss: -230.1134]\n",
      "2025-12-02 12:30:48,367: Epoch 57 average loss: avg_loss: -223.51692, avg_total_loss: -223.51692\n",
      "# Epoch 58: 100%|██████████| 22/22 [00:01<00:00, 20.15it/s, Loss: -212.6147]\n",
      "2025-12-02 12:30:49,465: Epoch 58 average loss: avg_loss: -222.31327, avg_total_loss: -222.31327\n",
      "# Epoch 58: 100%|██████████| 22/22 [00:01<00:00, 20.15it/s, Loss: -212.6147]\n",
      "2025-12-02 12:30:49,465: Epoch 58 average loss: avg_loss: -222.31327, avg_total_loss: -222.31327\n",
      "# Epoch 59: 100%|██████████| 22/22 [00:01<00:00, 20.50it/s, Loss: -234.9058]\n",
      "2025-12-02 12:30:50,543: Epoch 59 average loss: avg_loss: -225.57840, avg_total_loss: -225.57840\n",
      "2025-12-02 12:30:50,547: Setting learning rate to 0.0001\n",
      "# Epoch 59: 100%|██████████| 22/22 [00:01<00:00, 20.50it/s, Loss: -234.9058]\n",
      "2025-12-02 12:30:50,543: Epoch 59 average loss: avg_loss: -225.57840, avg_total_loss: -225.57840\n",
      "2025-12-02 12:30:50,547: Setting learning rate to 0.0001\n",
      "# Epoch 60: 100%|██████████| 22/22 [00:01<00:00, 20.64it/s, Loss: -189.6615]\n",
      "2025-12-02 12:30:51,615: Epoch 60 average loss: avg_loss: -233.11853, avg_total_loss: -233.11853\n",
      "# Epoch 60: 100%|██████████| 22/22 [00:01<00:00, 20.64it/s, Loss: -189.6615]\n",
      "2025-12-02 12:30:51,615: Epoch 60 average loss: avg_loss: -233.11853, avg_total_loss: -233.11853\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:06<00:00,  1.74s/it]\n",
      "\n",
      "2025-12-02 12:30:59,036: Epoch 60 average validation loss: 607.23281 -- Median validation metrics: avg_loss: 607.23281, NSE: -0.05229, KGE: 0.31905, Alpha-NSE: 0.58337, Beta-NSE: -0.41839\n",
      "# Epoch 61:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:30:59,036: Epoch 60 average validation loss: 607.23281 -- Median validation metrics: avg_loss: 607.23281, NSE: -0.05229, KGE: 0.31905, Alpha-NSE: 0.58337, Beta-NSE: -0.41839\n",
      "# Epoch 61: 100%|██████████| 22/22 [00:00<00:00, 23.07it/s, Loss: -255.0442]\n",
      "2025-12-02 12:30:59,991: Epoch 61 average loss: avg_loss: -241.72807, avg_total_loss: -241.72807\n",
      "# Epoch 61: 100%|██████████| 22/22 [00:00<00:00, 23.07it/s, Loss: -255.0442]\n",
      "2025-12-02 12:30:59,991: Epoch 61 average loss: avg_loss: -241.72807, avg_total_loss: -241.72807\n",
      "# Epoch 62: 100%|██████████| 22/22 [00:01<00:00, 19.84it/s, Loss: -209.7620]\n",
      "2025-12-02 12:31:01,105: Epoch 62 average loss: avg_loss: -242.99241, avg_total_loss: -242.99241\n",
      "# Epoch 62: 100%|██████████| 22/22 [00:01<00:00, 19.84it/s, Loss: -209.7620]\n",
      "2025-12-02 12:31:01,105: Epoch 62 average loss: avg_loss: -242.99241, avg_total_loss: -242.99241\n",
      "# Epoch 63: 100%|██████████| 22/22 [00:00<00:00, 23.15it/s, Loss: -264.2182]\n",
      "2025-12-02 12:31:02,060: Epoch 63 average loss: avg_loss: -245.41122, avg_total_loss: -245.41122\n",
      "# Epoch 63: 100%|██████████| 22/22 [00:00<00:00, 23.15it/s, Loss: -264.2182]\n",
      "2025-12-02 12:31:02,060: Epoch 63 average loss: avg_loss: -245.41122, avg_total_loss: -245.41122\n",
      "# Epoch 64: 100%|██████████| 22/22 [00:01<00:00, 19.89it/s, Loss: -238.4315]\n",
      "2025-12-02 12:31:03,171: Epoch 64 average loss: avg_loss: -246.54993, avg_total_loss: -246.54993\n",
      "# Epoch 64: 100%|██████████| 22/22 [00:01<00:00, 19.89it/s, Loss: -238.4315]\n",
      "2025-12-02 12:31:03,171: Epoch 64 average loss: avg_loss: -246.54993, avg_total_loss: -246.54993\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:07<00:00,  1.76s/it]\n",
      "\n",
      "2025-12-02 12:31:10,625: Epoch 64 average validation loss: 643.40222 -- Median validation metrics: avg_loss: 643.40222, NSE: -0.08308, KGE: 0.31407, Alpha-NSE: 0.58061, Beta-NSE: -0.42609\n",
      "# Epoch 65:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:31:10,625: Epoch 64 average validation loss: 643.40222 -- Median validation metrics: avg_loss: 643.40222, NSE: -0.08308, KGE: 0.31407, Alpha-NSE: 0.58061, Beta-NSE: -0.42609\n",
      "# Epoch 65: 100%|██████████| 22/22 [00:01<00:00, 20.35it/s, Loss: -252.1660]\n",
      "2025-12-02 12:31:11,708: Epoch 65 average loss: avg_loss: -247.88709, avg_total_loss: -247.88709\n",
      "# Epoch 65: 100%|██████████| 22/22 [00:01<00:00, 20.35it/s, Loss: -252.1660]\n",
      "2025-12-02 12:31:11,708: Epoch 65 average loss: avg_loss: -247.88709, avg_total_loss: -247.88709\n",
      "# Epoch 66: 100%|██████████| 22/22 [00:01<00:00, 20.14it/s, Loss: -271.0021]\n",
      "2025-12-02 12:31:12,805: Epoch 66 average loss: avg_loss: -248.82578, avg_total_loss: -248.82578\n",
      "# Epoch 66: 100%|██████████| 22/22 [00:01<00:00, 20.14it/s, Loss: -271.0021]\n",
      "2025-12-02 12:31:12,805: Epoch 66 average loss: avg_loss: -248.82578, avg_total_loss: -248.82578\n",
      "# Epoch 67: 100%|██████████| 22/22 [00:00<00:00, 22.99it/s, Loss: -231.3390]\n",
      "2025-12-02 12:31:13,767: Epoch 67 average loss: avg_loss: -248.95341, avg_total_loss: -248.95341\n",
      "# Epoch 67: 100%|██████████| 22/22 [00:00<00:00, 22.99it/s, Loss: -231.3390]\n",
      "2025-12-02 12:31:13,767: Epoch 67 average loss: avg_loss: -248.95341, avg_total_loss: -248.95341\n",
      "# Epoch 68: 100%|██████████| 22/22 [00:01<00:00, 20.46it/s, Loss: -263.5145]\n",
      "2025-12-02 12:31:14,847: Epoch 68 average loss: avg_loss: -250.83372, avg_total_loss: -250.83372\n",
      "# Epoch 68: 100%|██████████| 22/22 [00:01<00:00, 20.46it/s, Loss: -263.5145]\n",
      "2025-12-02 12:31:14,847: Epoch 68 average loss: avg_loss: -250.83372, avg_total_loss: -250.83372\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:06<00:00,  1.75s/it]\n",
      "\n",
      "2025-12-02 12:31:22,273: Epoch 68 average validation loss: 472.95785 -- Median validation metrics: avg_loss: 472.95785, NSE: 0.11529, KGE: 0.35203, Alpha-NSE: 0.60156, Beta-NSE: -0.29452\n",
      "# Epoch 69:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:31:22,273: Epoch 68 average validation loss: 472.95785 -- Median validation metrics: avg_loss: 472.95785, NSE: 0.11529, KGE: 0.35203, Alpha-NSE: 0.60156, Beta-NSE: -0.29452\n",
      "# Epoch 69: 100%|██████████| 22/22 [00:01<00:00, 19.12it/s, Loss: -233.4578]\n",
      "2025-12-02 12:31:23,426: Epoch 69 average loss: avg_loss: -250.63411, avg_total_loss: -250.63411\n",
      "# Epoch 69: 100%|██████████| 22/22 [00:01<00:00, 19.12it/s, Loss: -233.4578]\n",
      "2025-12-02 12:31:23,426: Epoch 69 average loss: avg_loss: -250.63411, avg_total_loss: -250.63411\n",
      "# Epoch 70: 100%|██████████| 22/22 [00:00<00:00, 22.74it/s, Loss: -265.3631]\n",
      "2025-12-02 12:31:24,400: Epoch 70 average loss: avg_loss: -252.39161, avg_total_loss: -252.39161\n",
      "# Epoch 70: 100%|██████████| 22/22 [00:00<00:00, 22.74it/s, Loss: -265.3631]\n",
      "2025-12-02 12:31:24,400: Epoch 70 average loss: avg_loss: -252.39161, avg_total_loss: -252.39161\n",
      "# Epoch 71: 100%|██████████| 22/22 [00:01<00:00, 20.04it/s, Loss: -269.9512]\n",
      "2025-12-02 12:31:25,503: Epoch 71 average loss: avg_loss: -252.97885, avg_total_loss: -252.97885\n",
      "# Epoch 71: 100%|██████████| 22/22 [00:01<00:00, 20.04it/s, Loss: -269.9512]\n",
      "2025-12-02 12:31:25,503: Epoch 71 average loss: avg_loss: -252.97885, avg_total_loss: -252.97885\n",
      "# Epoch 72: 100%|██████████| 22/22 [00:01<00:00, 19.85it/s, Loss: -247.7867]\n",
      "2025-12-02 12:31:26,616: Epoch 72 average loss: avg_loss: -253.43208, avg_total_loss: -253.43208\n",
      "# Epoch 72: 100%|██████████| 22/22 [00:01<00:00, 19.85it/s, Loss: -247.7867]\n",
      "2025-12-02 12:31:26,616: Epoch 72 average loss: avg_loss: -253.43208, avg_total_loss: -253.43208\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:06<00:00,  1.73s/it]\n",
      "\n",
      "2025-12-02 12:31:33,988: Epoch 72 average validation loss: 491.77460 -- Median validation metrics: avg_loss: 491.77460, NSE: 0.10883, KGE: 0.33459, Alpha-NSE: 0.58635, Beta-NSE: -0.32687\n",
      "# Epoch 73:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:31:33,988: Epoch 72 average validation loss: 491.77460 -- Median validation metrics: avg_loss: 491.77460, NSE: 0.10883, KGE: 0.33459, Alpha-NSE: 0.58635, Beta-NSE: -0.32687\n",
      "# Epoch 73: 100%|██████████| 22/22 [00:01<00:00, 19.73it/s, Loss: -272.6005]\n",
      "2025-12-02 12:31:35,104: Epoch 73 average loss: avg_loss: -255.20897, avg_total_loss: -255.20897\n",
      "# Epoch 73: 100%|██████████| 22/22 [00:01<00:00, 19.73it/s, Loss: -272.6005]\n",
      "2025-12-02 12:31:35,104: Epoch 73 average loss: avg_loss: -255.20897, avg_total_loss: -255.20897\n",
      "# Epoch 74: 100%|██████████| 22/22 [00:01<00:00, 20.29it/s, Loss: -269.7042]\n",
      "2025-12-02 12:31:36,194: Epoch 74 average loss: avg_loss: -255.74727, avg_total_loss: -255.74727\n",
      "# Epoch 74: 100%|██████████| 22/22 [00:01<00:00, 20.29it/s, Loss: -269.7042]\n",
      "2025-12-02 12:31:36,194: Epoch 74 average loss: avg_loss: -255.74727, avg_total_loss: -255.74727\n",
      "# Epoch 75: 100%|██████████| 22/22 [00:00<00:00, 22.89it/s, Loss: -257.2334]\n",
      "2025-12-02 12:31:37,161: Epoch 75 average loss: avg_loss: -256.34989, avg_total_loss: -256.34989\n",
      "# Epoch 75: 100%|██████████| 22/22 [00:00<00:00, 22.89it/s, Loss: -257.2334]\n",
      "2025-12-02 12:31:37,161: Epoch 75 average loss: avg_loss: -256.34989, avg_total_loss: -256.34989\n",
      "# Epoch 76: 100%|██████████| 22/22 [00:01<00:00, 19.68it/s, Loss: -269.2374]\n",
      "2025-12-02 12:31:38,283: Epoch 76 average loss: avg_loss: -256.82365, avg_total_loss: -256.82365\n",
      "# Epoch 76: 100%|██████████| 22/22 [00:01<00:00, 19.68it/s, Loss: -269.2374]\n",
      "2025-12-02 12:31:38,283: Epoch 76 average loss: avg_loss: -256.82365, avg_total_loss: -256.82365\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:07<00:00,  1.79s/it]\n",
      "\n",
      "2025-12-02 12:31:45,861: Epoch 76 average validation loss: 687.15284 -- Median validation metrics: avg_loss: 687.15284, NSE: -0.08927, KGE: 0.31524, Alpha-NSE: 0.59849, Beta-NSE: -0.42266\n",
      "# Epoch 77:   0%|          | 0/22 [00:00<?, ?it/s]2025-12-02 12:31:45,861: Epoch 76 average validation loss: 687.15284 -- Median validation metrics: avg_loss: 687.15284, NSE: -0.08927, KGE: 0.31524, Alpha-NSE: 0.59849, Beta-NSE: -0.42266\n",
      "# Epoch 77: 100%|██████████| 22/22 [00:00<00:00, 22.82it/s, Loss: -322.3966]\n",
      "2025-12-02 12:31:46,827: Epoch 77 average loss: avg_loss: -259.71400, avg_total_loss: -259.71400\n",
      "# Epoch 77: 100%|██████████| 22/22 [00:00<00:00, 22.82it/s, Loss: -322.3966]\n",
      "2025-12-02 12:31:46,827: Epoch 77 average loss: avg_loss: -259.71400, avg_total_loss: -259.71400\n",
      "# Epoch 78: 100%|██████████| 22/22 [00:01<00:00, 19.25it/s, Loss: -242.4312]\n",
      "2025-12-02 12:31:47,975: Epoch 78 average loss: avg_loss: -257.36812, avg_total_loss: -257.36812\n",
      "# Epoch 78: 100%|██████████| 22/22 [00:01<00:00, 19.25it/s, Loss: -242.4312]\n",
      "2025-12-02 12:31:47,975: Epoch 78 average loss: avg_loss: -257.36812, avg_total_loss: -257.36812\n",
      "# Epoch 79: 100%|██████████| 22/22 [00:01<00:00, 19.81it/s, Loss: -266.3965]\n",
      "2025-12-02 12:31:49,090: Epoch 79 average loss: avg_loss: -258.55925, avg_total_loss: -258.55925\n",
      "# Epoch 79: 100%|██████████| 22/22 [00:01<00:00, 19.81it/s, Loss: -266.3965]\n",
      "2025-12-02 12:31:49,090: Epoch 79 average loss: avg_loss: -258.55925, avg_total_loss: -258.55925\n",
      "# Epoch 80: 100%|██████████| 22/22 [00:00<00:00, 22.73it/s, Loss: -250.5647]\n",
      "2025-12-02 12:31:50,062: Epoch 80 average loss: avg_loss: -259.26984, avg_total_loss: -259.26984\n",
      "# Epoch 80: 100%|██████████| 22/22 [00:00<00:00, 22.73it/s, Loss: -250.5647]\n",
      "2025-12-02 12:31:50,062: Epoch 80 average loss: avg_loss: -259.26984, avg_total_loss: -259.26984\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "Metrics for 1h are calculated over last 1 elements only. Ignoring 239 predictions per sequence.\n",
      "# Validation: 100%|██████████| 4/4 [00:07<00:00,  1.77s/it]\n",
      "\n",
      "2025-12-02 12:31:57,594: Epoch 80 average validation loss: 696.77597 -- Median validation metrics: avg_loss: 696.77597, NSE: -0.10444, KGE: 0.31185, Alpha-NSE: 0.59959, Beta-NSE: -0.42710\n",
      "2025-12-02 12:31:57,594: Epoch 80 average validation loss: 696.77597 -- Median validation metrics: avg_loss: 696.77597, NSE: -0.10444, KGE: 0.31185, Alpha-NSE: 0.59959, Beta-NSE: -0.42710\n"
     ]
    }
   ],
   "source": [
    "# Capture the location of the Zarr cache verified in the previous cell\n",
    "# We need to pass this explicitly because we are about to reset train_dir\n",
    "if cfg.train_dir is not None:\n",
    "    zarr_cache_path = cfg.train_dir / \"train_data.zarr\"\n",
    "else:\n",
    "    zarr_cache_path = None\n",
    "\n",
    "# Modify config to use the Zarr cache and default run directory structure\n",
    "# CRITICAL: We set \"train_dir\": None. This forces BaseTrainer to create a NEW train_data directory\n",
    "# inside the new run directory, where the scaler will be saved.\n",
    "# If we left train_dir pointing to the old location, the scaler would be saved there,\n",
    "# but the validator would look for it in the new run directory, causing a FileNotFoundError.\n",
    "update_dict = {\n",
    "    \"train_dir\": None,\n",
    "    \"save_train_data\": False\n",
    "}\n",
    "\n",
    "if zarr_cache_path and zarr_cache_path.exists():\n",
    "    print(f\"Using existing Zarr cache at: {zarr_cache_path}\")\n",
    "    update_dict[\"train_data_file\"] = zarr_cache_path\n",
    "else:\n",
    "    print(\"Warning: Zarr cache not found. Dataset may be rebuilt.\")\n",
    "\n",
    "cfg.update_config(update_dict)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Starting run on GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    cfg.device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"Starting run on CPU\")\n",
    "    cfg.device = \"cpu\"\n",
    "\n",
    "start_training(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5895267",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Now that the model is trained, we evaluate it on the test set to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e80c09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest run directory\n",
    "run_dirs = glob.glob(\"runs/development_run_mdn*\")\n",
    "if not run_dirs:\n",
    "    raise FileNotFoundError(\"No run directories found in runs/\")\n",
    "# Sort by modification time to ensure we get the actual latest run\n",
    "# String sorting fails because the date format is DDMM_HHMMSS (Day first)\n",
    "run_dirs.sort(key=os.path.getmtime)\n",
    "latest_run_dir = Path(run_dirs[-1])\n",
    "print(f\"Evaluating run at: {latest_run_dir}\")\n",
    "\n",
    "# Run evaluation on test and train sets\n",
    "print(\"Starting Test Evaluation...\")\n",
    "eval_run(run_dir=latest_run_dir, period=\"test\")\n",
    "\n",
    "print(\"Starting Train Evaluation...\")\n",
    "eval_run(run_dir=latest_run_dir, period=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb863f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(latest_run_dir / \"test\" / \"model_epoch080\" / \"test_results.p\", \"rb\") as fp:\n",
    "    results = pickle.load(fp)\n",
    "    \n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88474a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['DE4']['1h']['xr']['discharge_vol_sim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e84bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the basin variable for the plots below\n",
    "# This should match one of the keys returned by results.keys()\n",
    "basin = 'DE2'\n",
    "print(f\"Basin set to: {basin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014060dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# --- Training Period: Day 1 Forecast Plot ---\n",
    "\n",
    "# 1. Load Training Results\n",
    "# We look for the train_results.p file in the run directory\n",
    "train_results_files = list(latest_run_dir.glob(\"train/*/train_results.p\"))\n",
    "if not train_results_files:\n",
    "    print(\"No training results found. Make sure eval_run(..., period='train') has completed.\")\n",
    "else:\n",
    "    train_results_file = train_results_files[0]\n",
    "    print(f\"Loading training results from: {train_results_file}\")\n",
    "    \n",
    "    with open(train_results_file, \"rb\") as fp:\n",
    "        train_results = pickle.load(fp)\n",
    "\n",
    "    if 'basin' in locals():\n",
    "        ds_train = train_results[basin]['1h']['xr']\n",
    "        \n",
    "        # 2. Subsample to avoid overlaps (Daily Stride)\n",
    "        dates_train = pd.to_datetime(ds_train['date'].values)\n",
    "        time_diff_train = dates_train[1] - dates_train[0]\n",
    "        \n",
    "        if time_diff_train < pd.Timedelta('24h'):\n",
    "            stride_train = int(pd.Timedelta('24h') / time_diff_train)\n",
    "            ds_train_subset = ds_train.isel(date=slice(0, None, stride_train))\n",
    "        else:\n",
    "            ds_train_subset = ds_train\n",
    "\n",
    "        # 3. Select Day 1 (First 24 hours)\n",
    "        ds_train_day1 = ds_train_subset.isel(time_step=slice(0, 24))\n",
    "\n",
    "        # 4. Stack\n",
    "        ds_train_flat = ds_train_day1.stack(combined=('date', 'time_step'))\n",
    "\n",
    "        # 5. Extract\n",
    "        if 'samples' in ds_train_flat['discharge_vol_sim'].dims:\n",
    "            qsim_train = ds_train_flat['discharge_vol_sim'].transpose('combined', 'samples').values\n",
    "        else:\n",
    "            qsim_train = ds_train_flat['discharge_vol_sim'].values\n",
    "        \n",
    "        qobs_train = ds_train_flat['discharge_vol_obs'].values\n",
    "\n",
    "        # 6. Reconstruct Dates\n",
    "        dates_train_flat = pd.to_datetime(ds_train_flat['date'].values) + pd.to_timedelta(ds_train_flat['time_step'].values, unit='h')\n",
    "        \n",
    "        # 7. Sort\n",
    "        sort_idx_train = np.argsort(dates_train_flat)\n",
    "        dates_train_flat = dates_train_flat[sort_idx_train]\n",
    "        qsim_train = qsim_train[sort_idx_train]\n",
    "        qobs_train = qobs_train[sort_idx_train]\n",
    "\n",
    "        # 8. Handle NaNs\n",
    "        nan_mask_train = np.all(np.isnan(qsim_train), axis=-1) if qsim_train.ndim == 2 else np.isnan(qsim_train)\n",
    "        if np.any(nan_mask_train):\n",
    "            valid_mask_train = ~nan_mask_train\n",
    "            dates_train_flat = dates_train_flat[valid_mask_train]\n",
    "            qsim_train = qsim_train[valid_mask_train]\n",
    "            qobs_train = qobs_train[valid_mask_train]\n",
    "\n",
    "        # 9. Calculate Percentiles\n",
    "        if qsim_train.ndim == 2:\n",
    "            y_median_train = np.nanmedian(qsim_train, axis=-1)\n",
    "            y_05_train = np.nanpercentile(qsim_train, 5, axis=-1)\n",
    "            y_95_train = np.nanpercentile(qsim_train, 95, axis=-1)\n",
    "            y_25_train = np.nanpercentile(qsim_train, 25, axis=-1)\n",
    "            y_75_train = np.nanpercentile(qsim_train, 75, axis=-1)\n",
    "        else:\n",
    "            y_median_train = qsim_train\n",
    "            y_05_train = y_95_train = y_25_train = y_75_train = qsim_train\n",
    "\n",
    "        # 10. Plot\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # 90% CI\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=np.concatenate([dates_train_flat, dates_train_flat[::-1]]),\n",
    "            y=np.concatenate([y_95_train, y_05_train[::-1]]),\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(53, 183, 121, 0.5)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            name='90% CI (5-95)',\n",
    "            showlegend=True\n",
    "        ))\n",
    "\n",
    "        # 50% CI\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=np.concatenate([dates_train_flat, dates_train_flat[::-1]]),\n",
    "            y=np.concatenate([y_75_train, y_25_train[::-1]]),\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(68, 1, 84, 0.5)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            name='50% CI (25-75)',\n",
    "            showlegend=True\n",
    "        ))\n",
    "\n",
    "        # Median\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=dates_train_flat,\n",
    "            y=y_median_train,\n",
    "            mode='lines',\n",
    "            line=dict(color='red', width=2),\n",
    "            name='Median'\n",
    "        ))\n",
    "\n",
    "        # Observed\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=dates_train_flat,\n",
    "            y=qobs_train,\n",
    "            mode='lines',\n",
    "            line=dict(color='black', width=2, dash='dash'),\n",
    "            name='Observed'\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title='Training Period: Discharge Prediction (Day 1 Ahead)',\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title='Discharge [m³/s]',\n",
    "            template='plotly_white',\n",
    "            hovermode='x unified'\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(\"Basin variable not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Data Preparation with Slider Support ---\n",
    "\n",
    "if 'results' in locals() and 'basin' in locals():\n",
    "    ds = results['DE2']['1h']['xr']\n",
    "    \n",
    "    # 1. Subsample to avoid overlaps (Daily Stride)\n",
    "    dates = pd.to_datetime(ds['date'].values)\n",
    "    time_diff = dates[1] - dates[0]\n",
    "    if time_diff < pd.Timedelta('24h'):\n",
    "        stride = int(pd.Timedelta('24h') / time_diff)\n",
    "        ds_subset = ds.isel(date=slice(0, None, stride))\n",
    "    else:\n",
    "        ds_subset = ds\n",
    "\n",
    "    # Determine number of days in forecast horizon\n",
    "    n_steps = len(ds_subset['time_step'])\n",
    "    n_days = n_steps // 24\n",
    "    print(f\"Generating interactive plot for {n_days} forecast days...\")\n",
    "\n",
    "    fig = go.Figure()\n",
    "    steps = []\n",
    "\n",
    "    # Loop through each day (0 to n_days-1) to generate traces\n",
    "    for day_idx in range(n_days):\n",
    "        # Select Day slice (e.g., 0-24, 24-48, etc.)\n",
    "        start_step = day_idx * 24\n",
    "        end_step = (day_idx + 1) * 24\n",
    "        \n",
    "        # Slice, Stack, Extract\n",
    "        ds_day = ds_subset.isel(time_step=slice(start_step, end_step))\n",
    "        ds_flat = ds_day.stack(combined=('date', 'time_step'))\n",
    "\n",
    "        if 'samples' in ds_flat['discharge_vol_sim'].dims:\n",
    "            qsim = ds_flat['discharge_vol_sim'].transpose('combined', 'samples').values\n",
    "        else:\n",
    "            qsim = ds_flat['discharge_vol_sim'].values\n",
    "        \n",
    "        qobs = ds_flat['discharge_vol_obs'].values\n",
    "\n",
    "        # Reconstruct Dates\n",
    "        dates_flat = pd.to_datetime(ds_flat['date'].values) + pd.to_timedelta(ds_flat['time_step'].values, unit='h')\n",
    "        \n",
    "        # Sort\n",
    "        sort_idx = np.argsort(dates_flat)\n",
    "        dates_flat = dates_flat[sort_idx]\n",
    "        qsim = qsim[sort_idx]\n",
    "        qobs = qobs[sort_idx]\n",
    "\n",
    "        # Handle NaNs\n",
    "        nan_mask = np.all(np.isnan(qsim), axis=-1) if qsim.ndim == 2 else np.isnan(qsim)\n",
    "        if np.any(nan_mask):\n",
    "            valid_mask = ~nan_mask\n",
    "            dates_flat = dates_flat[valid_mask]\n",
    "            qsim = qsim[valid_mask]\n",
    "            qobs = qobs[valid_mask]\n",
    "\n",
    "        # Calculate Percentiles\n",
    "        if qsim.ndim == 2:\n",
    "            y_median = np.nanmedian(qsim, axis=-1)\n",
    "            y_05 = np.nanpercentile(qsim, 5, axis=-1)\n",
    "            y_95 = np.nanpercentile(qsim, 95, axis=-1)\n",
    "            y_25 = np.nanpercentile(qsim, 25, axis=-1)\n",
    "            y_75 = np.nanpercentile(qsim, 75, axis=-1)\n",
    "        else:\n",
    "            y_median = qsim\n",
    "            y_05 = y_95 = y_25 = y_75 = qsim\n",
    "\n",
    "        # Visibility: Only Day 1 (index 0) is visible initially\n",
    "        is_visible = (day_idx == 0)\n",
    "\n",
    "        # Add Traces (4 traces per day)\n",
    "        \n",
    "        # 1. 90% CI\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=np.concatenate([dates_flat, dates_flat[::-1]]),\n",
    "            y=np.concatenate([y_95, y_05[::-1]]),\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(53, 183, 121, 0.5)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            name='90% CI (5-95)',\n",
    "            visible=is_visible,\n",
    "            showlegend=True\n",
    "        ))\n",
    "\n",
    "        # 2. 50% CI\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=np.concatenate([dates_flat, dates_flat[::-1]]),\n",
    "            y=np.concatenate([y_75, y_25[::-1]]),\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(68, 1, 84, 0.5)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            name='50% CI (25-75)',\n",
    "            visible=is_visible,\n",
    "            showlegend=True\n",
    "        ))\n",
    "\n",
    "        # 3. Median\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=dates_flat,\n",
    "            y=y_median,\n",
    "            mode='lines',\n",
    "            line=dict(color='red', width=2),\n",
    "            name='Median',\n",
    "            visible=is_visible,\n",
    "            showlegend=True\n",
    "        ))\n",
    "\n",
    "        # 4. Observed\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=dates_flat,\n",
    "            y=qobs,\n",
    "            mode='lines',\n",
    "            line=dict(color='black', width=2, dash='dash'),\n",
    "            name='Observed',\n",
    "            visible=is_visible,\n",
    "            showlegend=True\n",
    "        ))\n",
    "\n",
    "    # Create Slider Steps\n",
    "    # Total traces = n_days * 4\n",
    "    for i in range(n_days):\n",
    "        step = dict(\n",
    "            method=\"update\",\n",
    "            args=[{\"visible\": [False] * (n_days * 4)},\n",
    "                  {\"title\": f\"Discharge Prediction - Day {i + 1} Ahead\"}],\n",
    "            label=str(i + 1)\n",
    "        )\n",
    "        # Enable the 4 traces for this day\n",
    "        for j in range(4):\n",
    "            step[\"args\"][0][\"visible\"][i * 4 + j] = True\n",
    "        steps.append(step)\n",
    "\n",
    "    sliders = [dict(\n",
    "        active=0,\n",
    "        currentvalue={\"prefix\": \"Forecast Day: \"},\n",
    "        pad={\"t\": 50},\n",
    "        steps=steps\n",
    "    )]\n",
    "\n",
    "    fig.update_layout(\n",
    "        sliders=sliders,\n",
    "        title='Discharge Prediction - Day 1 Ahead',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Discharge [m³/s]',\n",
    "        template='plotly_white',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Please run the previous cells to load 'results' and define 'basin'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3d679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "\n",
    "# --- Spaghetti Plot: All Forecast Traces (Median) ---\n",
    "\n",
    "if 'results' in locals() and 'basin' in locals():\n",
    "    # Use ds_subset from previous cell if available, otherwise re-derive\n",
    "    if 'ds_subset' not in locals():\n",
    "        ds = results[basin]['1h']['xr']\n",
    "        dates = pd.to_datetime(ds['date'].values)\n",
    "        time_diff = dates[1] - dates[0]\n",
    "        if time_diff < pd.Timedelta('24h'):\n",
    "            stride = int(pd.Timedelta('24h') / time_diff)\n",
    "            ds_subset = ds.isel(date=slice(0, None, stride))\n",
    "        else:\n",
    "            ds_subset = ds\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # 1. Plot Individual Forecast Traces\n",
    "    n_forecasts = len(ds_subset['date'])\n",
    "    print(f\"Plotting {n_forecasts} forecast traces...\")\n",
    "    \n",
    "    # Use Plotly standard qualitative colors, cycling through them\n",
    "    palette = cycle(px.colors.qualitative.Plotly)\n",
    "    colors = [next(palette) for _ in range(n_forecasts)]\n",
    "\n",
    "    # Pre-calculate time steps in hours\n",
    "    time_steps_hours = pd.to_timedelta(ds_subset['time_step'].values, unit='h')\n",
    "    \n",
    "    for i, date in enumerate(ds_subset['date'].values):\n",
    "        # Extract forecast for this date\n",
    "        # Dimensions: (time_step, samples) -> we want median over samples\n",
    "        forecast_slice = ds_subset['discharge_vol_sim'].isel(date=i)\n",
    "        \n",
    "        if 'samples' in forecast_slice.dims:\n",
    "            # Calculate median across samples\n",
    "            forecast_median = forecast_slice.median(dim='samples').values\n",
    "        else:\n",
    "            forecast_median = forecast_slice.values\n",
    "            \n",
    "        # Construct x-axis (Date + Time Steps)\n",
    "        start_date = pd.to_datetime(date)\n",
    "        forecast_dates = start_date + time_steps_hours\n",
    "        \n",
    "        # Handle NaNs\n",
    "        if np.all(np.isnan(forecast_median)):\n",
    "            continue\n",
    "\n",
    "        # Add Trace with unique color from Plotly palette\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=forecast_dates,\n",
    "            y=forecast_median,\n",
    "            mode='lines',\n",
    "            line=dict(width=1.5, color=colors[i]), \n",
    "            opacity=0.8,\n",
    "            showlegend=False, \n",
    "            name=f'Forecast {start_date.strftime(\"%Y-%m-%d\")}'\n",
    "        ))\n",
    "\n",
    "    # 2. Plot Observations (Ground Truth)\n",
    "    # We reconstruct the observations from the first day (0-24h) of each forecast\n",
    "    # to ensure we have the full continuous time series starting from the first forecast date.\n",
    "    \n",
    "    obs_list = []\n",
    "    date_list = []\n",
    "    \n",
    "    for i in range(len(ds_subset['date'])):\n",
    "        # Take first 24 steps (Day 1) to stitch together the continuous timeline\n",
    "        obs_slice = ds_subset['discharge_vol_obs'].isel(date=i, time_step=slice(0, 24)).values\n",
    "        d_slice = pd.to_datetime(ds_subset['date'].values[i]) + time_steps_hours[:24]\n",
    "        \n",
    "        obs_list.append(obs_slice)\n",
    "        date_list.append(d_slice)\n",
    "        \n",
    "    flat_obs = np.concatenate(obs_list)\n",
    "    flat_dates = np.concatenate(date_list)\n",
    "    \n",
    "    # Sort to be sure\n",
    "    sort_idx = np.argsort(flat_dates)\n",
    "    flat_dates = flat_dates[sort_idx]\n",
    "    flat_obs = flat_obs[sort_idx]\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=flat_dates,\n",
    "        y=flat_obs,\n",
    "        mode='lines',\n",
    "        line=dict(color='black', width=2),\n",
    "        name='Observed'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='All Forecast Traces (Median) vs Observed',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Discharge [m³/s]',\n",
    "        template='plotly_white',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Please run the previous cells to load 'results' and define 'basin'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralhydrology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
