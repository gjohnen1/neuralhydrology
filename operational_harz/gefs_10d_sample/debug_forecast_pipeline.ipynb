{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "740233df",
   "metadata": {},
   "source": [
    "# Debug Forecast Dataset Pipeline\n",
    "\n",
    "This notebook walks through each step of the OnlineForecastDataset creation process,\n",
    "allowing you to inspect and visualize data at every stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f42f7328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Change to project root\n",
    "os.chdir('/home/sngrj0hn/GitHub/neuralhydrology')\n",
    "\n",
    "from neuralhydrology.utils.config import Config\n",
    "from neuralhydrology.datautils.fetch_basin_forecasts import (\n",
    "    load_basin_centroids,\n",
    "    fetch_forecasts_for_basins,\n",
    "    interpolate_to_hourly,\n",
    ")\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54ad465",
   "metadata": {},
   "source": [
    "## 1. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba12d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period: train\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'basins'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m cfg = Config(config_path)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPeriod: train\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBasins: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbasins\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mForecast inputs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.forecast_inputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHindcast inputs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.hindcast_inputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Config' object has no attribute 'basins'"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "config_path = Path('operational_harz/gefs_10d_sample/config.yml')\n",
    "cfg = Config(config_path)\n",
    "\n",
    "# Load basins from basins.txt\n",
    "basins_file = config_path.parent / 'basins.txt'\n",
    "with open(basins_file, 'r') as f:\n",
    "    basins = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Period: train\")\n",
    "print(f\"Basins (from {basins_file.name}): {basins}\")\n",
    "print(f\"Forecast inputs: {cfg.forecast_inputs}\")\n",
    "print(f\"Hindcast inputs: {cfg.hindcast_inputs}\")\n",
    "print(f\"Target variables: {cfg.target_variables}\")\n",
    "print(f\"\\nTrain period: {cfg.train_start_date} to {cfg.train_end_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e14e30",
   "metadata": {},
   "source": [
    "## 2. Extract Base Variable Names from Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988524bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip quartile suffixes to get base variable names\n",
    "base_vars_needed = set()\n",
    "for var in cfg.forecast_inputs:\n",
    "    base_var = var.replace('_q25', '').replace('_q50', '').replace('_q75', '')\n",
    "    base_vars_needed.add(base_var)\n",
    "\n",
    "print(f\"Config forecast_inputs (with quartile suffixes):\")\n",
    "for v in sorted(cfg.forecast_inputs):\n",
    "    print(f\"  - {v}\")\n",
    "\n",
    "print(f\"\\nBase variables needed from NOAA (without suffixes):\")\n",
    "for v in sorted(base_vars_needed):\n",
    "    print(f\"  - {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a5aa03",
   "metadata": {},
   "source": [
    "## 3. Connect to NOAA GEFS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7418ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Connecting to NOAA GEFS Zarr store...\")\n",
    "ds = xr.open_zarr(\n",
    "    \"https://data.dynamical.org/noaa/gefs/forecast-35-day/latest.zarr?email=optional@email.com\",\n",
    "    decode_timedelta=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Connected successfully\")\n",
    "print(f\"\\nDataset dimensions: {dict(ds.dims)}\")\n",
    "print(f\"\\nAvailable variables (first 20):\")\n",
    "for i, v in enumerate(list(ds.data_vars)[:20]):\n",
    "    print(f\"  {i+1}. {v}\")\n",
    "\n",
    "print(f\"\\nTime range: {ds.init_time.values[0]} to {ds.init_time.values[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a842f",
   "metadata": {},
   "source": [
    "## 4. Apply Temporal Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9765cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract period dates\n",
    "start_date = pd.to_datetime(cfg.train_start_date, format='%d/%m/%Y')\n",
    "end_date = pd.to_datetime(cfg.train_end_date, format='%d/%m/%Y')\n",
    "\n",
    "print(f\"Filtering to period: {start_date.date()} to {end_date.date()}\")\n",
    "\n",
    "# Temporal slice\n",
    "time_dim = 'init_time' if 'init_time' in ds.dims else 'time'\n",
    "ds_filtered = ds.sel({time_dim: slice(start_date, end_date)})\n",
    "\n",
    "print(f\"\\nBefore filtering: {len(ds[time_dim])} time steps\")\n",
    "print(f\"After filtering: {len(ds_filtered[time_dim])} time steps\")\n",
    "print(f\"Data reduction: {100 * (1 - len(ds_filtered[time_dim]) / len(ds[time_dim])):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87014733",
   "metadata": {},
   "source": [
    "## 5. Filter to Required Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b0c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which base variables are available\n",
    "available_base_vars = [v for v in base_vars_needed if v in ds_filtered.data_vars]\n",
    "missing_vars = [v for v in base_vars_needed if v not in ds_filtered.data_vars]\n",
    "\n",
    "print(f\"Requested base variables: {len(base_vars_needed)}\")\n",
    "print(f\"Available: {len(available_base_vars)}\")\n",
    "print(f\"Missing: {len(missing_vars)}\")\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\n⚠️ Missing variables:\")\n",
    "    for v in missing_vars:\n",
    "        print(f\"  - {v}\")\n",
    "\n",
    "print(f\"\\nFiltering to {len(available_base_vars)} variables...\")\n",
    "ds_filtered = ds_filtered[available_base_vars]\n",
    "\n",
    "print(f\"\\n✓ Dataset now contains: {list(ds_filtered.data_vars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0894568",
   "metadata": {},
   "source": [
    "## 6. Load Basin Centroids and Extract Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb3835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load centroids\n",
    "basin_centroids_file = cfg.data_dir / \"basin_centroids\" / \"basin_centroids.csv\"\n",
    "centroids = load_basin_centroids(basin_centroids_file)\n",
    "\n",
    "print(f\"Loaded {len(centroids)} basin centroids\")\n",
    "print(f\"\\nCentroids:\")\n",
    "print(centroids)\n",
    "\n",
    "# Filter to configured basins (from basins.txt)\n",
    "centroids = centroids[centroids['basin_name'].isin(basins)]\n",
    "print(f\"\\nFiltered to {len(centroids)} configured basins\")\n",
    "\n",
    "# Extract forecasts for basin locations\n",
    "print(f\"\\nExtracting forecasts for basin centroids...\")\n",
    "basin_forecasts = fetch_forecasts_for_basins(ds_filtered, centroids)\n",
    "\n",
    "print(f\"\\n✓ Basin forecasts extracted\")\n",
    "print(f\"Dimensions: {dict(basin_forecasts.dims)}\")\n",
    "print(f\"Variables: {list(basin_forecasts.data_vars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba240df",
   "metadata": {},
   "source": [
    "## 7. Compute Ensemble Quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c201ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "quartiles = [0.25, 0.5, 0.75]\n",
    "quartile_suffixes = {0.25: '_q25', 0.5: '_q50', 0.75: '_q75'}\n",
    "\n",
    "print(f\"Computing quartiles {quartiles} from {len(basin_forecasts.ensemble_member)} ensemble members...\")\n",
    "\n",
    "new_data_vars = {}\n",
    "for var_name in basin_forecasts.data_vars:\n",
    "    var_data = basin_forecasts[var_name]\n",
    "    var_quartiles = var_data.quantile(quartiles, dim='ensemble_member')\n",
    "    \n",
    "    for i, q in enumerate(quartiles):\n",
    "        suffix = quartile_suffixes.get(q, f'_q{int(q*100)}')\n",
    "        new_var_name = f\"{var_name}{suffix}\"\n",
    "        quartile_data = var_quartiles.isel(quantile=i).drop('quantile')\n",
    "        new_data_vars[new_var_name] = quartile_data\n",
    "\n",
    "coords_to_keep = {k: v for k, v in basin_forecasts.coords.items() \n",
    "                 if 'ensemble_member' not in v.dims}\n",
    "\n",
    "basin_forecasts_quartiles = xr.Dataset(\n",
    "    data_vars=new_data_vars,\n",
    "    coords=coords_to_keep,\n",
    "    attrs=basin_forecasts.attrs.copy()\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Computed {len(new_data_vars)} quartile variables\")\n",
    "print(f\"Dimensions: {dict(basin_forecasts_quartiles.dims)}\")\n",
    "print(f\"\\nQuartile variables created:\")\n",
    "for v in sorted(basin_forecasts_quartiles.data_vars):\n",
    "    print(f\"  - {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eddecd",
   "metadata": {},
   "source": [
    "## 8. Interpolate to Hourly Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f83bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Interpolating to hourly for first 240 hours...\")\n",
    "print(f\"\\nBefore interpolation:\")\n",
    "print(f\"  Lead times: {len(basin_forecasts_quartiles.lead_time)}\")\n",
    "print(f\"  Lead time range: {basin_forecasts_quartiles.lead_time.values[0]} to {basin_forecasts_quartiles.lead_time.values[-1]}\")\n",
    "\n",
    "basin_forecasts_hourly = interpolate_to_hourly(basin_forecasts_quartiles, max_hours=240)\n",
    "\n",
    "print(f\"\\nAfter interpolation:\")\n",
    "print(f\"  Lead times: {len(basin_forecasts_hourly.lead_time)}\")\n",
    "print(f\"  Lead time range: {basin_forecasts_hourly.lead_time.values[0]} to {basin_forecasts_hourly.lead_time.values[-1]}\")\n",
    "print(f\"\\n✓ Interpolation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8351a",
   "metadata": {},
   "source": [
    "## 9. Final Variable Filtering and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to exact config variables\n",
    "forecast_vars = [var for var in basin_forecasts_hourly.data_vars \n",
    "                if var in cfg.forecast_inputs]\n",
    "\n",
    "print(f\"Config expects {len(cfg.forecast_inputs)} forecast variables\")\n",
    "print(f\"Dataset has {len(forecast_vars)} matching variables\")\n",
    "\n",
    "missing_in_dataset = [v for v in cfg.forecast_inputs if v not in basin_forecasts_hourly.data_vars]\n",
    "if missing_in_dataset:\n",
    "    print(f\"\\n⚠️ Variables in config but missing from dataset:\")\n",
    "    for v in missing_in_dataset:\n",
    "        print(f\"  - {v}\")\n",
    "\n",
    "if not forecast_vars:\n",
    "    print(f\"\\n❌ ERROR: No matching variables found!\")\n",
    "    print(f\"\\nConfig forecast_inputs: {cfg.forecast_inputs}\")\n",
    "    print(f\"\\nDataset variables: {list(basin_forecasts_hourly.data_vars)}\")\n",
    "else:\n",
    "    basin_forecasts_hourly = basin_forecasts_hourly[forecast_vars]\n",
    "    print(f\"\\n✓ Filtered to {len(forecast_vars)} variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c4e6c8",
   "metadata": {},
   "source": [
    "## 10. Visualize Sample Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9554e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if forecast_vars:\n",
    "    # Pick first variable and first basin\n",
    "    sample_var = forecast_vars[0]\n",
    "    sample_basin = basin_forecasts_hourly.basin.values[0]\n",
    "    \n",
    "    print(f\"Plotting: {sample_var} for basin {sample_basin}\")\n",
    "    \n",
    "    # Select data for first 5 forecast initialization times\n",
    "    sample_data = basin_forecasts_hourly[sample_var].sel(basin=sample_basin).isel(time=slice(0, 5))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    for i, t in enumerate(sample_data.time.values[:5]):\n",
    "        forecast = sample_data.sel(time=t)\n",
    "        lead_hours = sample_data.lead_time.values\n",
    "        ax.plot(lead_hours, forecast.values, marker='o', markersize=2, \n",
    "               label=f\"Init: {pd.to_datetime(t).strftime('%Y-%m-%d %H:%M')}\")\n",
    "    \n",
    "    ax.set_xlabel('Lead Time (hours)')\n",
    "    ax.set_ylabel(sample_var)\n",
    "    ax.set_title(f'{sample_var} Forecasts for Basin {sample_basin}\\n(First 5 initialization times)')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✓ Visualization complete\")\n",
    "else:\n",
    "    print(\"Cannot visualize - no forecast variables available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853adb25",
   "metadata": {},
   "source": [
    "## 11. Check Data Chunking (Before Compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking chunking status...\\n\")\n",
    "for var in list(basin_forecasts_hourly.data_vars)[:3]:  # Check first 3 variables\n",
    "    data = basin_forecasts_hourly[var].data\n",
    "    if hasattr(data, 'chunks'):\n",
    "        print(f\"{var}:\")\n",
    "        print(f\"  Type: Dask array\")\n",
    "        print(f\"  Chunks: {data.chunks}\")\n",
    "    else:\n",
    "        print(f\"{var}:\")\n",
    "        print(f\"  Type: Numpy array (in-memory)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c7878c",
   "metadata": {},
   "source": [
    "## 12. Materialize Data (Compute)\n",
    "\n",
    "⚠️ This step will download all data from the remote server.\n",
    "Monitor network activity and RAM usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a445b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Rechunk for parallel processing\n",
    "if 'basin' in basin_forecasts_hourly.dims:\n",
    "    print(\"Rechunking for parallel computation...\")\n",
    "    basin_forecasts_hourly = basin_forecasts_hourly.chunk(\n",
    "        {'basin': 1, 'time': -1, 'lead_time': -1}\n",
    "    )\n",
    "    print(f\"✓ Rechunked\")\n",
    "\n",
    "print(f\"\\nStarting compute (materialization)...\")\n",
    "print(f\"This will download ~{basin_forecasts_hourly.nbytes / 1e9:.2f} GB of data\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "basin_forecasts_hourly = basin_forecasts_hourly.compute(\n",
    "    scheduler='threads', \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Compute complete in {elapsed:.1f} seconds\")\n",
    "print(f\"Download speed: ~{(basin_forecasts_hourly.nbytes / 1e6) / elapsed:.1f} MB/s\")\n",
    "\n",
    "# Verify it's now in-memory\n",
    "sample_var = list(basin_forecasts_hourly.data_vars)[0]\n",
    "print(f\"\\nVerifying data type:\")\n",
    "print(f\"  {sample_var}: {type(basin_forecasts_hourly[sample_var].data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68395e4",
   "metadata": {},
   "source": [
    "## 13. Final Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584daddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINAL FORECAST DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDimensions: {dict(basin_forecasts_hourly.dims)}\")\n",
    "print(f\"\\nCoordinates:\")\n",
    "for coord in basin_forecasts_hourly.coords:\n",
    "    print(f\"  - {coord}: {basin_forecasts_hourly[coord].shape}\")\n",
    "\n",
    "print(f\"\\nData Variables ({len(basin_forecasts_hourly.data_vars)}):\")\n",
    "for var in sorted(basin_forecasts_hourly.data_vars):\n",
    "    shape = basin_forecasts_hourly[var].shape\n",
    "    size_mb = basin_forecasts_hourly[var].nbytes / 1e6\n",
    "    print(f\"  - {var}: {shape} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(f\"\\nTotal size: {basin_forecasts_hourly.nbytes / 1e9:.2f} GB\")\n",
    "print(f\"Memory type: In-memory numpy arrays\")\n",
    "print(f\"\\n✓ Forecast dataset ready for merging with historical data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a94a97",
   "metadata": {},
   "source": [
    "## 14. Save for Later Use (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beedf13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save the processed forecast dataset\n",
    "# output_file = Path('operational_harz/gefs_10d_sample/forecast_debug_output.nc')\n",
    "# print(f\"Saving to {output_file}...\")\n",
    "# basin_forecasts_hourly.to_netcdf(output_file)\n",
    "# print(f\"✓ Saved ({output_file.stat().st_size / 1e6:.1f} MB)\")\n",
    "\n",
    "print(\"To save, uncomment the code above\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralhydrology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
